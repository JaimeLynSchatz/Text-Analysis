{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Document Similarity with Latent Semantic Analysis (LSA)\n",
    "\n",
    "The following notebook walks you through doing LSA document similarity in Python. We then output the document similarity matrix as a .csv file which can be manipulated to highlight similarity between documents. We also output a heatmap which gives an initial impression of the similarity between documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/master/Intro/Python/Py_notebooks/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specificaly for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different pakcages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary packages for notebook \n",
    "\n",
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download. Make sure to have the following packages installed before beginning so that they can be accessed while running the scripts.\n",
    "\n",
    "In your terminal, packages can be installed by simply typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:  \n",
    "\n",
    "- **sklearn:** Simple and efficient tools for data mining and data analysis built on NumPy, SciPy, and matplotlib.\n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- **warnings:** Allows for the manipulation of warning messages in Python.\n",
    "- **numpy:** a general-purpose array-processing package designed to efficiently manipulate large multi-dimensional arrays of arbitrary records without sacrificing too much speed for small multi-dimensional arrays. \n",
    "- **re:** Provides regular expression matching operations similar to those found in Perl.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "- **string:** Contains a number of functions to process standard Python strings.\n",
    "- **nltk:** A leading platform for building Python programs to work with human language data.\n",
    "- **seaborn:** a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "- **matplotlib:** Produces publication quality 2D graphics for interactive graphing, scientific publishing, user interface development and web application servers targeting multiple user interfaces and hardcopy output formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# import all of the scikit learn stuff\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import re\n",
    "from os.path import join, isfile, splitext\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will ignore deprecation and future warnings. All the warnings in this code are not concerning and will not break the code or cause errors in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppress warnings from pandas library\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
    "                        module=\"pandas\", lineno=570)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
    "                        module = \"sklearn\", lineno = 1059)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File paths\n",
    "Here we are saving as variables different file paths that we need in our code. We do this so that they are easier to call later and so that you can make most of your changes now and not need to make as many changes later. \n",
    "\n",
    "First we use the `os` package above to find our `[\"HOME\"]` directory using the `environ` function. This will work for any operating system, so if you decide to try this out on your personal computer instead of ReD, the `homePath` variable will still be the path to your 'home' directory, so no changes are needed.\n",
    "\n",
    "Next, we combine the `homePath` variable with the folder names that lead to where our data is stored. Note that we do not use any file names yet, just the path to the folder. This is because we are comparing documents to one another, so we need to read in an entire directory. You will want to change the folder names to match your folder names in your file path.\n",
    "\n",
    "Now we add the `homePath` variable to other folder names that lead to a folder where we will want to save any output generated by this code. You again will want to change the file names in the path to match your own file names. We save this file path as the variable `dataResults`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homePath = os.environ[\"HOME\"]\n",
    "dataHome = os.path.join(homePath, \"Text-Analysis-master\", \"data\", \"shakespeareDated\")\n",
    "dataResults = os.path.join(homePath, \"Text-Analysis-master\", \"Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set needed variables\n",
    "\n",
    "Now we are setting needed variables that will help determine what the code will do farther down. Just like the file path variables, this is done so you have to make fewer changes later and also to make the changes easier to find by putting them in one place.\n",
    "\n",
    "If you want to use the stopword list that comes with the nltk package then set `nltkStop` equal to **True**. If you do not wish to use the nltk stopword list then set `nltkStop` equal to **False**.\n",
    "\n",
    "If you have created your own custom stopword list and wish to use that, then set `customStop` equal to **True**. If you do not have your own custom stopword list then set `customStop` equal to **False**.\n",
    "\n",
    "Now we choose the language we will be using for the nltk stopwords list and the nltk stemmer. If you need a different language, simply change 'english' (keep the quotes) in the `language` variable to the anglicized name of the language you wish to use (e.g. 'spanish' instead of 'espanol' or 'german' instead of 'deutsch').\n",
    "\n",
    "**NOTE: You can use both the nltk and custom stopword lists or you can use neither or just one or the other. You do NOT need to set them both to True or both to False. Use whatever works best for you.**\n",
    "\n",
    "The `stopWords =[]` variable is simply an empty list. This is where the words from the nltk stopword list or your custom stopword list or both combined or neither (depending on what you decide) will reside later on. You do not need to do anything to this line of code.\n",
    "\n",
    "The `tokenDict = {}` variable is an empty dictionary. This is where your documents will reside later. The file name for the document will be the key and the content of the document will be the value. This will be explained in more detail later. For now, you do not need to do anything to this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltkStop = True\n",
    "customStop = True\n",
    "language = 'english'\n",
    "stopWords = []\n",
    "tokenDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "If you set `nltkStop` equal to **True** above then this will add the NLTK stopwords list to the empty list named `stopWords`.\n",
    "\n",
    "You should have already chosen your desired language above, but if you wish to add any words to the stopWords list then add the word(s) you want as a stop word in the `stopWords.extend(['words', 'you', 'want', 'to', 'add'])` part of the code.\n",
    "\n",
    "If you need to see the list of available languages in nltk simply remove the `#` from in front of the last line and run the cell. A list of available languages will print out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if nltkStop is True:\n",
    "    # NLTK Stop words\n",
    "    stopWords = stopwords.words(language)\n",
    "\n",
    "    stopWords.extend(['would', 'said', 'says', 'also'])\n",
    "    #print (\" \".join(stopwords.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add own stopword list\n",
    "\n",
    "Here is where your own stopwords list is added if you set `customStop` equal to **True** above. Here you will need to change the folder names and file name to match your folders and file. Remember to put each folder name in quotes and in the correct order always putting the file name including the file extension (.txt) last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if customStop is True:\n",
    "    stopWordsFilepath = os.path.join(homePath, \"Text-Analysis-master\", \"data\", \"earlyModernStopword.txt\")\n",
    "\n",
    "    with open(stopWordsFilepath, \"r\",encoding = 'utf-8') as f:\n",
    "        stopWordsList = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    stopWords.extend(stopWordsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary and stemmer\n",
    "Here we save the word stemmer we are using as the variable `stemmer`. \n",
    "\n",
    "The stemmer will truncate endings of words that end in 'ing' or 's' or 'es' et cetera. This will help ensure that words such as 'reading' and 'read' are considered as the same word so that the concept of 'to read' is given equal consideration. You should have already chosen your desired language up above.\n",
    "\n",
    "A list of languages available with the nltk SnowballStemmer can be found by removing the `#` infront of the last line of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language)\n",
    "#print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We need to create a function in order to stem and tokenize our data. Any time you see `def` that means we are **DE**claring a **F**unction. The `def` is usually followed by the name of the function being created and then in parentheses are the parameters required by the function. After the parentheses is a colon, which closes the declaration, then a bunch of code below which is indented. The indented code is the program statement or statements to be executed. Once you have created your function all you need to do in order to run it is call the function by name and make sure you have included all the required parameters in the parentheses. This allows you to call the function without having to write out all the code in the function every time you wish to perform that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and tokenization functions\n",
    "Here we create two functions. The first is where we will use the `stemmer` variable above to create a function that will stem the words in our documents when applied.\n",
    "\n",
    "The second tokenizes our documents which means it splits it into individual words. This function uses the `stemTokens` function as part of it so later in the code we will only need to apply the `tokenize` function. \n",
    "\n",
    "You should not need to make any changes to this block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemTokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stemTokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in documents\n",
    "Now we read in our documents and also perform some text cleaning. This code lower cases all the words as well as removes punctuation. Then it adds the file names and cleaned content of each file to our previously empty `tokenDict` dictionary above. You should not need to make any changes to this code.\n",
    "\n",
    "A dictionary is similar to a list except it has what are called 'keys' and 'values'. This basically allows us to label our data. In this case we will be making the file names of our documents the 'keys' and the content of the file the 'values' so that each document name correlates to the content of that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for subdir, dirs, files in os.walk(dataHome):\n",
    "    for file in files:\n",
    "        if file.startswith('.'):\n",
    "                continue\n",
    "        filePath = subdir + os.path.sep + file\n",
    "        with open(filePath, 'r', encoding = 'ISO-8859-1') as textFile:\n",
    "            text = textFile.read()\n",
    "            lowers = text.lower()\n",
    "            noPunctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "            tokenDict[file] = noPunctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check and see if our dictionary now has our data. We are asking to see the first 10 keys of our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1589TwoGentlemenOfVerona.txt', '1590TamingOfTheShrew.txt', '1591KingHenry6_1.txt', '1591KingHenry6_2.txt', '1591KingHenry6_3.txt', '1591TitusAndronicus.txt', '1592KingRichard3.txt', '1594ComedyOfErrors.txt', '1594LovesLaboursLost.txt', '1595KingRichard2.txt']\n"
     ]
    }
   ],
   "source": [
    "print(list(tokenDict.keys())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer\n",
    "\n",
    "Here we weight the importance of each word in the document. This is done using Term Frequency-Inverse Document Frequency (Tfidf). This considers how important a word is based on the frequency in the whole corpus as well as in individual documents. This allows for words that might not have a high frequency in an entire collection, but do have a high frequency in one or two documents when compared to other words to still be given a higher level of importance throughout the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becaus', 'befor', 'besid', 'dure', 'els', 'elsewher', 'everi', 'everyon', 'everyth', 'everywher', 'fifi', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', \"n't\", 'need', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'say', 'sever', 'sha', 'sinc', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = tokenize, stop_words=stopWords)\n",
    "dtm = vectorizer.fit_transform(tokenDict.values())\n",
    "testDF = pd.DataFrame(dtm.toarray(), index=tokenDict.keys(), columns = vectorizer.get_feature_names())\n",
    "testDF = testDF.sort_index(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our data frame where the rows are the documents and the columns are the terms. To adjust which part of the data frame you see in the output change the numbers in the square brackets. To see the 10th thru the 20th row and the 100th to 110th column the numbers in the brackets should read `[10:20, 100:110]`. So the first set of numbers is for what rows you wish to see and the second set dictate the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>massacr</th>\n",
       "      <th>massi</th>\n",
       "      <th>mast</th>\n",
       "      <th>master</th>\n",
       "      <th>mastercord</th>\n",
       "      <th>masterdom</th>\n",
       "      <th>mastergunn</th>\n",
       "      <th>masterleav</th>\n",
       "      <th>masterless</th>\n",
       "      <th>masterpiec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1589TwoGentlemenOfVerona.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590TamingOfTheShrew.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591KingHenry6_1.txt</th>\n",
       "      <td>0.015566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591KingHenry6_2.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591KingHenry6_3.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006464</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591TitusAndronicus.txt</th>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592KingRichard3.txt</th>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>0.012907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594ComedyOfErrors.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.146888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594LovesLaboursLost.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595KingRichard2.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               massacr  massi      mast    master  mastercord  \\\n",
       "1589TwoGentlemenOfVerona.txt  0.000000    0.0  0.000000  0.079115         0.0   \n",
       "1590TamingOfTheShrew.txt      0.000000    0.0  0.000000  0.104583         0.0   \n",
       "1591KingHenry6_1.txt          0.015566    0.0  0.000000  0.012101         0.0   \n",
       "1591KingHenry6_2.txt          0.000000    0.0  0.000000  0.051486         0.0   \n",
       "1591KingHenry6_3.txt          0.000000    0.0  0.006464  0.010712         0.0   \n",
       "1591TitusAndronicus.txt       0.009424    0.0  0.000000  0.006279         0.0   \n",
       "1592KingRichard3.txt          0.008609    0.0  0.003462  0.012907         0.0   \n",
       "1594ComedyOfErrors.txt        0.000000    0.0  0.013904  0.146888         0.0   \n",
       "1594LovesLaboursLost.txt      0.000000    0.0  0.000000  0.059978         0.0   \n",
       "1595KingRichard2.txt          0.000000    0.0  0.000000  0.003632         0.0   \n",
       "\n",
       "                              masterdom  mastergunn  masterleav  masterless  \\\n",
       "1589TwoGentlemenOfVerona.txt        0.0    0.000000         0.0         0.0   \n",
       "1590TamingOfTheShrew.txt            0.0    0.000000         0.0         0.0   \n",
       "1591KingHenry6_1.txt                0.0    0.006773         0.0         0.0   \n",
       "1591KingHenry6_2.txt                0.0    0.000000         0.0         0.0   \n",
       "1591KingHenry6_3.txt                0.0    0.000000         0.0         0.0   \n",
       "1591TitusAndronicus.txt             0.0    0.000000         0.0         0.0   \n",
       "1592KingRichard3.txt                0.0    0.000000         0.0         0.0   \n",
       "1594ComedyOfErrors.txt              0.0    0.000000         0.0         0.0   \n",
       "1594LovesLaboursLost.txt            0.0    0.000000         0.0         0.0   \n",
       "1595KingRichard2.txt                0.0    0.000000         0.0         0.0   \n",
       "\n",
       "                              masterpiec  \n",
       "1589TwoGentlemenOfVerona.txt         0.0  \n",
       "1590TamingOfTheShrew.txt             0.0  \n",
       "1591KingHenry6_1.txt                 0.0  \n",
       "1591KingHenry6_2.txt                 0.0  \n",
       "1591KingHenry6_3.txt                 0.0  \n",
       "1591TitusAndronicus.txt              0.0  \n",
       "1592KingRichard3.txt                 0.0  \n",
       "1594ComedyOfErrors.txt               0.0  \n",
       "1594LovesLaboursLost.txt             0.0  \n",
       "1595KingRichard2.txt                 0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.iloc[:10, 7640:7650]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code outputs all of the words that make up the columns once we have broken our corpus down into a Tfidf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2d',\n",
       " '2s',\n",
       " '4d',\n",
       " '5s',\n",
       " '6d',\n",
       " '8d',\n",
       " 'aaron',\n",
       " 'abandon',\n",
       " 'abas',\n",
       " 'abash',\n",
       " 'abat',\n",
       " 'abatfowl',\n",
       " 'abbess',\n",
       " 'abbey',\n",
       " 'abbot',\n",
       " 'abbrevi',\n",
       " 'abc',\n",
       " 'abe',\n",
       " 'abel',\n",
       " 'abergavenni',\n",
       " 'abet',\n",
       " 'abhor',\n",
       " 'abhorredst',\n",
       " 'abhorson',\n",
       " 'abi',\n",
       " 'abid',\n",
       " 'abil',\n",
       " 'abird',\n",
       " 'abject',\n",
       " 'abjur',\n",
       " 'abl',\n",
       " 'ableed',\n",
       " 'abler',\n",
       " 'aboard',\n",
       " 'abod',\n",
       " 'abomin',\n",
       " 'abort',\n",
       " 'abound',\n",
       " 'abov',\n",
       " 'abraham',\n",
       " 'abram',\n",
       " 'abreast',\n",
       " 'abreed',\n",
       " 'abrew',\n",
       " 'abridg',\n",
       " 'abroach',\n",
       " 'abroad',\n",
       " 'abrog',\n",
       " 'abrupt',\n",
       " 'absenc',\n",
       " 'absent',\n",
       " 'abseybook',\n",
       " 'absolut',\n",
       " 'absolv',\n",
       " 'abstain',\n",
       " 'abstemi',\n",
       " 'abstin',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'absyrti',\n",
       " 'abund',\n",
       " 'abus',\n",
       " 'abut',\n",
       " 'abysm',\n",
       " 'academ',\n",
       " 'acapr',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessori',\n",
       " 'accid',\n",
       " 'accident',\n",
       " 'accit',\n",
       " 'acclam',\n",
       " 'accommod',\n",
       " 'accompani',\n",
       " 'accomplic',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'accost',\n",
       " 'account',\n",
       " 'accountst',\n",
       " 'accouter',\n",
       " 'accoutr',\n",
       " 'accru',\n",
       " 'accumul',\n",
       " 'accurs',\n",
       " 'accus',\n",
       " 'accustom',\n",
       " 'ace',\n",
       " 'ach',\n",
       " 'acheron',\n",
       " 'achiev',\n",
       " 'achill',\n",
       " 'achitophel',\n",
       " 'acknowledg',\n",
       " 'acknown',\n",
       " 'acold',\n",
       " 'acom',\n",
       " 'aconitum',\n",
       " 'acorn',\n",
       " 'acquaint',\n",
       " 'acquir',\n",
       " 'acquisit',\n",
       " 'acquit',\n",
       " 'acquitt',\n",
       " 'acr',\n",
       " 'act',\n",
       " 'acteon',\n",
       " 'action',\n",
       " 'actiontak',\n",
       " 'actium',\n",
       " 'activ',\n",
       " 'activevali',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'acurs',\n",
       " 'acut',\n",
       " 'ad',\n",
       " 'adag',\n",
       " 'adalla',\n",
       " 'adam',\n",
       " 'aday',\n",
       " 'add',\n",
       " 'adder',\n",
       " 'addict',\n",
       " 'addit',\n",
       " 'addl',\n",
       " 'address',\n",
       " 'adher',\n",
       " 'adi',\n",
       " 'adieu',\n",
       " 'adieus',\n",
       " 'adjac',\n",
       " 'adjoin',\n",
       " 'adjourn',\n",
       " 'adjudg',\n",
       " 'adjunct',\n",
       " 'administ',\n",
       " 'administr',\n",
       " 'admir',\n",
       " 'admit',\n",
       " 'admitt',\n",
       " 'admonish',\n",
       " 'admonit',\n",
       " 'ado',\n",
       " 'adoni',\n",
       " 'adopt',\n",
       " 'adopti',\n",
       " 'ador',\n",
       " 'adorn',\n",
       " 'adown',\n",
       " 'adowna',\n",
       " 'adramadio',\n",
       " 'adrian',\n",
       " 'adriana',\n",
       " 'adriano',\n",
       " 'adriat',\n",
       " 'aduck',\n",
       " 'adul',\n",
       " 'adulter',\n",
       " 'adulteress',\n",
       " 'adulteri',\n",
       " 'adultress',\n",
       " 'advanc',\n",
       " 'advantag',\n",
       " 'adventr',\n",
       " 'adventur',\n",
       " 'advers',\n",
       " 'adversari',\n",
       " 'advertis',\n",
       " 'advic',\n",
       " 'advis',\n",
       " 'advoc',\n",
       " 'aeacid',\n",
       " 'aedil',\n",
       " 'aegl',\n",
       " 'aemilius',\n",
       " 'aenea',\n",
       " 'aeolus',\n",
       " 'aeri',\n",
       " 'aerial',\n",
       " 'aesculapius',\n",
       " 'aeson',\n",
       " 'aesop',\n",
       " 'afar',\n",
       " 'afear',\n",
       " 'afeast',\n",
       " 'affabl',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affection',\n",
       " 'affeer',\n",
       " 'affi',\n",
       " 'affianc',\n",
       " 'affin',\n",
       " 'affirm',\n",
       " 'afflict',\n",
       " 'afford',\n",
       " 'affray',\n",
       " 'affright',\n",
       " 'affront',\n",
       " 'afield',\n",
       " 'afir',\n",
       " 'afloat',\n",
       " 'afoot',\n",
       " 'aforehand',\n",
       " 'aforesaid',\n",
       " 'afraid',\n",
       " 'afresh',\n",
       " 'afric',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'afront',\n",
       " 'afterdebt',\n",
       " 'afterdinn',\n",
       " 'afterey',\n",
       " 'afterhour',\n",
       " 'afterinquiri',\n",
       " 'afterlov',\n",
       " 'aftermeet',\n",
       " 'afternoon',\n",
       " 'aftersupp',\n",
       " 'aftertim',\n",
       " 'afterward',\n",
       " 'agamemnon',\n",
       " 'agat',\n",
       " 'agaz',\n",
       " 'age',\n",
       " 'agenor',\n",
       " 'agent',\n",
       " 'aggrav',\n",
       " 'aggrief',\n",
       " 'agil',\n",
       " 'agincourt',\n",
       " 'agit',\n",
       " 'agletbabi',\n",
       " 'ago',\n",
       " 'agon',\n",
       " 'agoni',\n",
       " 'agood',\n",
       " 'agre',\n",
       " 'agreement',\n",
       " 'agrippa',\n",
       " 'aground',\n",
       " 'agrow',\n",
       " 'agu',\n",
       " 'aguecheek',\n",
       " 'aguefac',\n",
       " 'agueproof',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahang',\n",
       " 'aheight',\n",
       " 'ahigh',\n",
       " 'ahold',\n",
       " 'ahoot',\n",
       " 'ahorseback',\n",
       " 'ahungri',\n",
       " 'aid',\n",
       " 'aidanc',\n",
       " 'aidant',\n",
       " 'aidless',\n",
       " 'ail',\n",
       " 'ailst',\n",
       " 'aim',\n",
       " 'air',\n",
       " 'airbrav',\n",
       " 'airdrawn',\n",
       " 'airi',\n",
       " 'airless',\n",
       " 'ajax',\n",
       " 'akil',\n",
       " 'ala',\n",
       " 'alabast',\n",
       " 'alack',\n",
       " 'alacr',\n",
       " 'alanson',\n",
       " 'alarbus',\n",
       " 'alarm',\n",
       " 'alban',\n",
       " 'albani',\n",
       " 'albeit',\n",
       " 'albion',\n",
       " 'alchemi',\n",
       " 'alchemist',\n",
       " 'alcibiad',\n",
       " 'alcid',\n",
       " 'alderliefest',\n",
       " 'alderman',\n",
       " 'aldermen',\n",
       " 'ale',\n",
       " 'alecto',\n",
       " 'alehous',\n",
       " 'alencon',\n",
       " 'aleppo',\n",
       " 'alewash',\n",
       " 'alewif',\n",
       " 'alexa',\n",
       " 'alexand',\n",
       " 'alexandria',\n",
       " 'alexandrian',\n",
       " 'alia',\n",
       " 'alic',\n",
       " 'alien',\n",
       " 'aliena',\n",
       " 'alif',\n",
       " 'alight',\n",
       " 'alik',\n",
       " 'alisand',\n",
       " 'aliv',\n",
       " 'allabhor',\n",
       " 'alladmir',\n",
       " 'allay',\n",
       " 'allchang',\n",
       " 'allcheer',\n",
       " 'alldisgrac',\n",
       " 'alldread',\n",
       " 'alleg',\n",
       " 'allegi',\n",
       " 'allend',\n",
       " 'alley',\n",
       " 'allhail',\n",
       " 'allhallond',\n",
       " 'allhallowma',\n",
       " 'allhallown',\n",
       " 'allhat',\n",
       " 'allhonor',\n",
       " 'alli',\n",
       " 'allianc',\n",
       " 'allicholi',\n",
       " 'allig',\n",
       " 'alllicens',\n",
       " 'allnobl',\n",
       " 'allobey',\n",
       " 'allot',\n",
       " 'allotteri',\n",
       " 'allow',\n",
       " 'allprais',\n",
       " 'allse',\n",
       " 'allseer',\n",
       " 'allshak',\n",
       " 'allshun',\n",
       " 'alltel',\n",
       " 'allth',\n",
       " 'allun',\n",
       " 'allur',\n",
       " 'allus',\n",
       " 'allwatch',\n",
       " 'allworthi',\n",
       " 'allycholli',\n",
       " 'alm',\n",
       " 'almain',\n",
       " 'almanac',\n",
       " 'almighti',\n",
       " 'almond',\n",
       " 'almsbasket',\n",
       " 'almsde',\n",
       " 'almsdrink',\n",
       " 'almshous',\n",
       " 'almsman',\n",
       " 'aloft',\n",
       " 'alon',\n",
       " 'alonso',\n",
       " 'aloof',\n",
       " 'aloud',\n",
       " 'alp',\n",
       " 'alphabet',\n",
       " 'alphonso',\n",
       " 'alreadi',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'althaea',\n",
       " 'althea',\n",
       " 'altitud',\n",
       " 'altogeth',\n",
       " 'alton',\n",
       " 'altr',\n",
       " 'alway',\n",
       " 'alwayswindobey',\n",
       " 'amaimon',\n",
       " 'amain',\n",
       " 'amak',\n",
       " 'amamon',\n",
       " 'amaz',\n",
       " 'amazed',\n",
       " 'amazon',\n",
       " 'amazonian',\n",
       " 'ambassador',\n",
       " 'amber',\n",
       " 'ambercolor',\n",
       " 'ambigu',\n",
       " 'ambit',\n",
       " 'ambiti',\n",
       " 'ambl',\n",
       " 'ambsac',\n",
       " 'ambuscado',\n",
       " 'ambush',\n",
       " 'ameer',\n",
       " 'amen',\n",
       " 'amend',\n",
       " 'amerc',\n",
       " 'america',\n",
       " 'amiabl',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'amien',\n",
       " 'amiss',\n",
       " 'amiti',\n",
       " 'amor',\n",
       " 'amorn',\n",
       " 'amort',\n",
       " 'amphimachus',\n",
       " 'ampl',\n",
       " 'ampler',\n",
       " 'amplest',\n",
       " 'ampli',\n",
       " 'amplifi',\n",
       " 'ampthil',\n",
       " 'amurath',\n",
       " 'amynta',\n",
       " 'anatom',\n",
       " 'anatomi',\n",
       " 'ancestor',\n",
       " 'ancestri',\n",
       " 'anchis',\n",
       " 'anchor',\n",
       " 'anchorag',\n",
       " 'anchovi',\n",
       " 'ancient',\n",
       " 'ancientri',\n",
       " 'ancientst',\n",
       " 'ancus',\n",
       " 'andiron',\n",
       " 'andren',\n",
       " 'andrew',\n",
       " 'andromach',\n",
       " 'andronici',\n",
       " 'andronicus',\n",
       " 'anew',\n",
       " 'angel',\n",
       " 'angelica',\n",
       " 'angellik',\n",
       " 'angelo',\n",
       " 'anger',\n",
       " 'angier',\n",
       " 'angl',\n",
       " 'angler',\n",
       " 'anglish',\n",
       " 'angri',\n",
       " 'angrili',\n",
       " 'anguish',\n",
       " 'angus',\n",
       " 'anhungri',\n",
       " 'ani',\n",
       " 'anight',\n",
       " 'anim',\n",
       " 'anjou',\n",
       " 'ankl',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annal',\n",
       " 'annex',\n",
       " 'annothan',\n",
       " 'annoy',\n",
       " 'annual',\n",
       " 'anoint',\n",
       " 'anon',\n",
       " 'anoth',\n",
       " 'anselm',\n",
       " 'answer',\n",
       " 'ant',\n",
       " 'antenor',\n",
       " 'antenorid',\n",
       " 'anthem',\n",
       " 'anthoni',\n",
       " 'anthropophagi',\n",
       " 'anthropophaginian',\n",
       " 'antiat',\n",
       " 'antic',\n",
       " 'anticip',\n",
       " 'antick',\n",
       " 'antidot',\n",
       " 'antigonus',\n",
       " 'antiopa',\n",
       " 'antipathi',\n",
       " 'antipholus',\n",
       " 'antipod',\n",
       " 'antiqu',\n",
       " 'antiquari',\n",
       " 'antium',\n",
       " 'antoni',\n",
       " 'antoniad',\n",
       " 'antonio',\n",
       " 'antonius',\n",
       " 'antr',\n",
       " 'anvil',\n",
       " 'anybodi',\n",
       " 'anymor',\n",
       " 'anyon',\n",
       " 'anypodi',\n",
       " 'anyth',\n",
       " 'anywher',\n",
       " 'ap',\n",
       " 'apac',\n",
       " 'apart',\n",
       " 'ape',\n",
       " 'apebear',\n",
       " 'apemantus',\n",
       " 'apennin',\n",
       " 'apiec',\n",
       " 'apish',\n",
       " 'apollo',\n",
       " 'apollodorus',\n",
       " 'apolog',\n",
       " 'apoplex',\n",
       " 'apoplexi',\n",
       " 'apostl',\n",
       " 'apostrophus',\n",
       " 'apothecari',\n",
       " 'appal',\n",
       " 'appar',\n",
       " 'apparel',\n",
       " 'apparit',\n",
       " 'appeach',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appeas',\n",
       " 'appel',\n",
       " 'appendix',\n",
       " 'apperil',\n",
       " 'appertain',\n",
       " 'appetit',\n",
       " 'appl',\n",
       " 'applaud',\n",
       " 'applaus',\n",
       " 'applejohn',\n",
       " 'appli',\n",
       " 'applianc',\n",
       " 'applic',\n",
       " 'appoint',\n",
       " 'apprehend',\n",
       " 'apprehens',\n",
       " 'apprentic',\n",
       " 'apprenticehood',\n",
       " 'approach',\n",
       " 'approb',\n",
       " 'approof',\n",
       " 'appropri',\n",
       " 'approv',\n",
       " 'appurten',\n",
       " 'apray',\n",
       " 'apricok',\n",
       " 'apricot',\n",
       " 'april',\n",
       " 'apron',\n",
       " 'apronmen',\n",
       " 'apt',\n",
       " 'apter',\n",
       " 'aptest',\n",
       " 'aqua',\n",
       " 'aquavita',\n",
       " 'aquilon',\n",
       " 'aquitain',\n",
       " 'arabia',\n",
       " 'arabian',\n",
       " 'aragon',\n",
       " 'arais',\n",
       " 'arbitr',\n",
       " 'arbitra',\n",
       " 'arbor',\n",
       " 'arc',\n",
       " 'arch',\n",
       " 'archbishop',\n",
       " 'archbishopr',\n",
       " 'archdeacon',\n",
       " 'archelaus',\n",
       " 'archenemi',\n",
       " 'archer',\n",
       " 'archeri',\n",
       " 'archheret',\n",
       " 'archibald',\n",
       " 'architect',\n",
       " 'archmock',\n",
       " 'archon',\n",
       " 'archvillain',\n",
       " 'ard',\n",
       " 'arden',\n",
       " 'ardent',\n",
       " 'ardor',\n",
       " 'arepair',\n",
       " 'argal',\n",
       " 'argier',\n",
       " 'argo',\n",
       " 'argosi',\n",
       " 'argu',\n",
       " 'argument',\n",
       " 'argus',\n",
       " 'ari',\n",
       " 'ariachn',\n",
       " 'ariadn',\n",
       " 'ariel',\n",
       " 'aright',\n",
       " 'arion',\n",
       " 'aripen',\n",
       " 'aris',\n",
       " 'aristotl',\n",
       " 'arithmet',\n",
       " 'arithmetician',\n",
       " 'ark',\n",
       " 'arm',\n",
       " 'arma',\n",
       " 'armada',\n",
       " 'armado',\n",
       " 'armagnac',\n",
       " 'armenia',\n",
       " 'armgaunt',\n",
       " 'armi',\n",
       " 'armigero',\n",
       " 'arminarm',\n",
       " 'armipot',\n",
       " 'armour',\n",
       " 'armouri',\n",
       " 'aroint',\n",
       " 'arol',\n",
       " 'aros',\n",
       " 'arous',\n",
       " 'arow',\n",
       " 'arra',\n",
       " 'arragon',\n",
       " 'arraign',\n",
       " 'arrant',\n",
       " 'array',\n",
       " 'arrearag',\n",
       " 'arrest',\n",
       " 'arriv',\n",
       " 'arrog',\n",
       " 'arrow',\n",
       " 'art',\n",
       " 'artemidorus',\n",
       " 'arthur',\n",
       " 'articl',\n",
       " 'articul',\n",
       " 'artific',\n",
       " 'artifici',\n",
       " 'artilleri',\n",
       " 'artist',\n",
       " 'artless',\n",
       " 'artoi',\n",
       " 'artsman',\n",
       " 'artur',\n",
       " 'arviragus',\n",
       " 'ascanius',\n",
       " 'ascend',\n",
       " 'ascens',\n",
       " 'ascent',\n",
       " 'ascorn',\n",
       " 'ascrib',\n",
       " 'ash',\n",
       " 'asham',\n",
       " 'asher',\n",
       " 'ashford',\n",
       " 'ashi',\n",
       " 'ashor',\n",
       " 'ashout',\n",
       " 'asia',\n",
       " 'asid',\n",
       " 'asinego',\n",
       " 'ask',\n",
       " 'askanc',\n",
       " 'askant',\n",
       " 'asker',\n",
       " 'asleep',\n",
       " 'asmath',\n",
       " 'aspect',\n",
       " 'aspen',\n",
       " 'aspers',\n",
       " 'aspic',\n",
       " 'aspici',\n",
       " 'aspir',\n",
       " 'asquint',\n",
       " 'ass',\n",
       " 'assail',\n",
       " 'assassin',\n",
       " 'assault',\n",
       " 'assay',\n",
       " 'assembl',\n",
       " 'assent',\n",
       " 'asshead',\n",
       " 'assign',\n",
       " 'assist',\n",
       " 'associ',\n",
       " 'asss',\n",
       " 'assuag',\n",
       " 'assubjug',\n",
       " 'assum',\n",
       " 'assur',\n",
       " 'assyrian',\n",
       " 'astonish',\n",
       " 'astraea',\n",
       " 'astray',\n",
       " 'astronom',\n",
       " 'asund',\n",
       " 'aswear',\n",
       " 'atalanta',\n",
       " 'atalk',\n",
       " 'ate',\n",
       " 'athen',\n",
       " 'athenian',\n",
       " 'athol',\n",
       " 'athversari',\n",
       " 'athwart',\n",
       " 'atilt',\n",
       " 'atlas',\n",
       " 'atomi',\n",
       " 'aton',\n",
       " 'atropo',\n",
       " 'attach',\n",
       " 'attain',\n",
       " 'attaind',\n",
       " 'attaint',\n",
       " 'attaintur',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attent',\n",
       " 'attest',\n",
       " 'attir',\n",
       " 'attorney',\n",
       " 'attorneyship',\n",
       " 'attract',\n",
       " 'attribut',\n",
       " 'atwain',\n",
       " 'aubrey',\n",
       " 'auburn',\n",
       " 'audac',\n",
       " 'audaci',\n",
       " 'audibl',\n",
       " 'audienc',\n",
       " 'audit',\n",
       " 'auditor',\n",
       " 'auditori',\n",
       " 'audrey',\n",
       " 'auf',\n",
       " 'aufidius',\n",
       " 'auger',\n",
       " 'aught',\n",
       " 'augment',\n",
       " 'augur',\n",
       " 'auguri',\n",
       " 'august',\n",
       " 'augustus',\n",
       " 'auld',\n",
       " 'aumerl',\n",
       " 'aunt',\n",
       " 'auntmoth',\n",
       " 'auricular',\n",
       " 'aurora',\n",
       " 'auspici',\n",
       " 'auster',\n",
       " 'austria',\n",
       " 'authent',\n",
       " 'author',\n",
       " 'autolycus',\n",
       " 'autumn',\n",
       " 'auvergn',\n",
       " 'avail',\n",
       " 'avaric',\n",
       " 'avarici',\n",
       " 'avaunt',\n",
       " 'ave',\n",
       " 'aveng',\n",
       " 'aver',\n",
       " 'avert',\n",
       " 'avis',\n",
       " 'avoid',\n",
       " 'avoirdupoi',\n",
       " 'avouch',\n",
       " 'avow',\n",
       " 'aw',\n",
       " 'await',\n",
       " 'awak',\n",
       " 'awaken',\n",
       " 'award',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'aweep',\n",
       " 'aweless',\n",
       " 'awhil',\n",
       " 'awkward',\n",
       " 'awl',\n",
       " 'awoo',\n",
       " 'awork',\n",
       " 'awri',\n",
       " 'ax',\n",
       " 'axe',\n",
       " 'axletre',\n",
       " 'ay',\n",
       " 'aye',\n",
       " 'ayll',\n",
       " 'azur',\n",
       " 'azurelac',\n",
       " 'baa',\n",
       " 'babbl',\n",
       " 'babe',\n",
       " 'babi',\n",
       " 'baboon',\n",
       " 'babylon',\n",
       " 'bacchan',\n",
       " 'bacchus',\n",
       " 'bachelor',\n",
       " 'bachlorship',\n",
       " 'backbit',\n",
       " 'backbitten',\n",
       " 'backdoor',\n",
       " 'backfriend',\n",
       " 'backsid',\n",
       " 'backsword',\n",
       " 'backtrick',\n",
       " 'backward',\n",
       " 'backwound',\n",
       " 'bacon',\n",
       " 'baconf',\n",
       " 'bad',\n",
       " 'bade',\n",
       " 'badg',\n",
       " 'baffl',\n",
       " 'bag',\n",
       " 'baggag',\n",
       " 'bagot',\n",
       " 'bagpip',\n",
       " 'bail',\n",
       " 'bailiff',\n",
       " 'bairn',\n",
       " 'bait',\n",
       " 'bajazeth',\n",
       " 'bake',\n",
       " 'baker',\n",
       " 'balanc',\n",
       " 'bald',\n",
       " 'baldpat',\n",
       " 'baldrick',\n",
       " 'bale',\n",
       " 'balk',\n",
       " 'ball',\n",
       " 'ballad',\n",
       " 'balladmak',\n",
       " 'balladmong',\n",
       " 'ballast',\n",
       " 'ballow',\n",
       " 'balm',\n",
       " 'balmi',\n",
       " 'balsam',\n",
       " 'balsamum',\n",
       " 'balthazar',\n",
       " 'ban',\n",
       " 'banburi',\n",
       " 'band',\n",
       " 'bandi',\n",
       " 'banditto',\n",
       " 'bandog',\n",
       " 'bane',\n",
       " 'bang',\n",
       " 'banish',\n",
       " 'banist',\n",
       " 'bank',\n",
       " 'bankrupt',\n",
       " 'bann',\n",
       " 'banner',\n",
       " 'banneret',\n",
       " 'banquet',\n",
       " 'banquo',\n",
       " 'baptis',\n",
       " 'baptism',\n",
       " 'baptista',\n",
       " 'bar',\n",
       " 'barabba',\n",
       " 'barb',\n",
       " 'barbar',\n",
       " 'barbari',\n",
       " 'barbarian',\n",
       " 'barbason',\n",
       " 'barber',\n",
       " 'barbermong',\n",
       " 'bard',\n",
       " 'bardolph',\n",
       " 'bare',\n",
       " 'barebon',\n",
       " 'barefac',\n",
       " 'barefoot',\n",
       " 'baregnawn',\n",
       " 'barehead',\n",
       " 'barepick',\n",
       " 'barerib',\n",
       " 'barg',\n",
       " 'bargain',\n",
       " 'bargulus',\n",
       " 'bark',\n",
       " 'barki',\n",
       " 'barklough',\n",
       " 'barkst',\n",
       " 'barley',\n",
       " 'barm',\n",
       " 'barn',\n",
       " 'barnacl',\n",
       " 'barnardin',\n",
       " 'barnardo',\n",
       " 'barnet',\n",
       " 'baron',\n",
       " 'baroni',\n",
       " 'barrel',\n",
       " 'barren',\n",
       " 'barrenspirit',\n",
       " 'barricad',\n",
       " 'barricado',\n",
       " 'barrow',\n",
       " 'barson',\n",
       " 'barter',\n",
       " 'bartholomew',\n",
       " 'bartholomewtid',\n",
       " 'basan',\n",
       " 'base',\n",
       " 'baseborn',\n",
       " 'baseless',\n",
       " 'baser',\n",
       " 'basest',\n",
       " 'bash',\n",
       " 'basi',\n",
       " 'basiliscolik',\n",
       " 'basilisk',\n",
       " 'basimecu',\n",
       " 'basin',\n",
       " 'basingstok',\n",
       " 'bask',\n",
       " 'basket',\n",
       " 'baskethilt',\n",
       " 'bass',\n",
       " 'bassanio',\n",
       " 'bassianus',\n",
       " 'bast',\n",
       " 'bastard',\n",
       " 'bastardi',\n",
       " 'basterd',\n",
       " 'bastinado',\n",
       " 'bat',\n",
       " 'bate',\n",
       " 'bath',\n",
       " 'battalia',\n",
       " 'battalion',\n",
       " 'batten',\n",
       " 'batter',\n",
       " 'batteri',\n",
       " 'batti',\n",
       " 'battl',\n",
       " 'battleax',\n",
       " 'battlement',\n",
       " 'battler',\n",
       " 'baubl',\n",
       " 'bavin',\n",
       " 'bawcock',\n",
       " 'bawd',\n",
       " 'bawdi',\n",
       " 'bawdri',\n",
       " 'bawl',\n",
       " 'bay',\n",
       " 'baynard',\n",
       " 'bayonn',\n",
       " 'beach',\n",
       " 'beachi',\n",
       " 'beacon',\n",
       " 'bead',\n",
       " 'beadl',\n",
       " 'beadsman',\n",
       " 'beadsmen',\n",
       " 'beagl',\n",
       " 'beak',\n",
       " 'beall',\n",
       " 'beam',\n",
       " 'bean',\n",
       " 'beanf',\n",
       " 'bear',\n",
       " 'bearbait',\n",
       " 'beard',\n",
       " 'beardless',\n",
       " 'bearer',\n",
       " 'bearherd',\n",
       " 'bearingcloth',\n",
       " 'bearlik',\n",
       " 'bearwhelp',\n",
       " 'beast',\n",
       " 'beastli',\n",
       " 'beastliest',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beatric',\n",
       " 'beau',\n",
       " 'beaufort',\n",
       " 'beaumont',\n",
       " 'beauteous',\n",
       " 'beauti',\n",
       " 'beautifi',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words that correspond to each column\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SVD and Cosine Similarity\n",
    "\n",
    "Here we run our Tfidf matrix created above through Singular Value Decomposition and then calculate the Cosine Similarity of the documents to one another. \n",
    "\n",
    "Singular Value Decomposition condenses our Tfidf matrix down a bit to make it easier to process. Here we also set the number of dimensions (`n_components`) , how many times we iterate over the corpus (`n_iter`), and then set the seed (`random_state`) so that the results are reproducable. At the moment the `random_state` is set to 42 which sets the seed for the random number generator, but feel free to adjust the number to get a slightly different output. Just make sure you keep the seed the same once you find one you like for reproducibility.\n",
    "\n",
    "Cosine similarity is where we measure how similar the documents are to one another. The result is a number between -1 and 1 with 1 being a perfect match (which we will get when the document is compared to itself) and -1 being completely different which we might get if we have a document of all numbers and one of all words with no numbers at all. Usually, even documents that are about unrelated topics share some common words and so are not completely dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components = 100, n_iter = 1000, random_state = 42)\n",
    "dtmLsa = lsa.fit_transform(dtm)\n",
    "cosineSim = cosine_similarity(dtmLsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as a data frame\n",
    "\n",
    "Now we save the results as a data frame. First we name the output .csv file so it matches our data. We do this in the first line of the cell.\n",
    "\n",
    "Next we add color based on the numbers in the data frame cells, so right now we have it set so that the more dissimilar the more red the cell will be and the more similar the more green it will be with those in the middle looking more yellow. You can adjust the colors by changing the `colorChoice` variable to something else. Options for colors can be found here: [https://matplotlib.org/tutorials/colors/colormaps.html](https://matplotlib.org/tutorials/colors/colormaps.html) \n",
    "\n",
    "Then we create the data frame and say we want the rows and columns to be labeled with the file names. Then we sort the columns in alphanumeric order by column header, then we sort the rows alphanumericaly by row label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvFileName = \"docSimilarityMatrix.csv\"\n",
    "colorChoice = 'RdYlGn'\n",
    "\n",
    "df = pd.DataFrame(cosineSim, index = tokenDict.keys(), columns=tokenDict.keys())\n",
    "dfS = df[sorted(df)]\n",
    "sortedDf = dfS.sort_index(axis = 0)\n",
    "sortedDf.to_csv(os.path.join(dataResults, csvFileName))\n",
    "#colorDf = sortedDf.style.background_gradient(cmap=colorChoice)\n",
    "#colorDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Heatmap\n",
    "\n",
    "Now we plot a heatmap and save it as a .png file for use in a paper, presentation, or even a website. \n",
    "\n",
    "First, we name the output file of the heatmap which is in the variable `heatmapFileName`. \n",
    "\n",
    "Next we choose the dpi level we want the output to be. We have it set to 300 which is generally good for both print and website use. \n",
    "\n",
    "Then we choose the color scheme we want. At the moment we have one where green represents the highest number (most similar) and red represents the lowest numbers (least similar) with yellow being more in the middle. If another color scheme would work better for you other options for color schemes can be found here: [https://matplotlib.org/tutorials/colors/colormaps.html](https://matplotlib.org/tutorials/colors/colormaps.html)\n",
    "\n",
    "Now we choose the font size. This will depend on your data and the size of your heatmap, so you'll want to play around with the number in `fontScale` until you find a value that works with your heatmap. \n",
    "\n",
    "\n",
    "Now we plot our heatmap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Variables\n",
    "#heatmapFileName = 'DocSimHeatmap.svg'\n",
    "#dpi = 300\n",
    "#colorScheme = 'RdYlGn'\n",
    "#fontScale = 5\n",
    "\n",
    "# Plot\n",
    "#figureSize = len(sortedDf)\n",
    "#sns.set(rc={'figure.figsize':(figureSize + 10, figureSize)}, font_scale = fontScale)\n",
    "#ax = sns.heatmap(sortedDf, cmap = colorScheme)\n",
    "#ax.figure.savefig(os.path.join(dataResults, heatmapFileName), dpi = dpi, bbox_inches='tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOILA!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was adapted from https://www.datascienceassn.org/sites/default/files/users/user1/lsa_presentation_final.pdf at Colorado University, Boulder. Accessed on 02/01/2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
