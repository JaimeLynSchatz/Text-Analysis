{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-Occurrence of Words in an individual Shakespeare Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following co-occurrence script aims to discover the proximity of words to a word of interest in an individual Shakespeare play. At the end, it will take in a word of the user's choice and find the words that co-occur within a chosen distance (here, distance being the number of words before and after the chosen word, so if you choose 10, it will look at the 10 words before and the 10 words after the word of interest). The script tells how many times each word appears within the specified distance from the chosen word, the furthest distance each word is from the chosen word (within the specified limit), the shortest distance of each word from the chosen word, and the overall average distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to have set up a [Karst account](https://kb.iu.edu/d/bezu#account) first. Once you have your Karst account simply go to [rstudio.iu.edu](https://rstudio.iu.edu/auth-sign-in) and login using your IU username and passphrase.  Next, set the working directory by pointing to the location on Karst where you have stored the files. Below, we have chosen to save the folder \"Text-Analysis\" as a \"Project\" in R Studio on the Karst super-computer here at Indiana University. It contains the R scripts, texts, notebooks, and results. If you have forked and cloned the Github repository (see [textPrep.Rmd](textPrep.Rmd) for directions on how), simply point to where you have saved the folder. If you save it to your personal Karst folder, it will most likely look very similar to the example below. Karst is a unix server and so the home directory is represented by a ~ and, thus, the path will look like this \"~/Text-Analysis/\" (with the quotes). Alternatively, if you are on a PC, you will need to use an absolute path such as \"C:/Users/XXX\" (with the quotes again).\n",
    "\n",
    "In R Studio, click Session in the menu bar > Set Working Directory > Choose Directory, then select the Text-Analysis directory in which you are working. This will set your working directory in the console pane, but make sure to copy the path into the source pane above to keep the directory constant if you close this script and reopen later. Make sure you click on the blue cube with a \"R\" in the center to set your working directory to your Text-Analysis project path.\n",
    "\n",
    "HINT: Your working directory is the folder from which you will be pulling your texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setwd(\"~/Text-Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include necessary packages for notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of R, others created by R users are available for download. Make sure to have the following packages installed before beginning so that they can be accessed while running the scripts.\n",
    "\n",
    "In R Studio, packages can be installed by navigating to Tools in the menu bar > Install Packages. Or in the bottom right panel click on the \"packages\" tab and then click on \"install.\"\n",
    "\n",
    "The packages are used within the co-occurrence script:\n",
    "\n",
    "tidytext - Text mining for word processing and sentiment analysis using 'dplyr', 'ggplot2', and other tidy tools.\n",
    "\n",
    "dplyr - tool for working with data frame like objects, both in memory and out of memory.\n",
    "\n",
    "fuzzyjoin - Join tables together based not on whether columns match exactly, but whether they are similar by some comparison. Implementations include string distance and regular expression matching.\n",
    "\n",
    "tm - this package provides tools (functions) for performing various types of text mining. In this script, we will use tm to perform text cleaning in order to have uniform data for analysis. Check out [this link](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) for the documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Loading required package: NLP\n"
     ]
    }
   ],
   "source": [
    "library(tidytext)\n",
    "library(dplyr)\n",
    "library(fuzzyjoin)\n",
    "library(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus <- scan(\"data/shakespeareFolger/Hamlet.txt\", what=\"character\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create early modern stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myStopWords <- scan(\"data/earlyModernStopword.txt\", what=\"character\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the corpus by:\n",
    "- Lower casing every word (so 'Love' and 'love' are counted as the same word).\n",
    "- Removing punctuation.\n",
    "- Striping whitespace (This means removing gaps larger than a single space).\n",
    "- Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycorpus <- tolower(corpus)\n",
    "mycorpus <- removePunctuation(mycorpus)\n",
    "mycorpus <- stripWhitespace(mycorpus)\n",
    "mycorpus <- removeWords(mycorpus, myStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the corpus into a data.frame where each row is one word. Then add a position column, and lastly remove regular english stopwords (different from our earlymodern stopword list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words <- data_frame(text = mycorpus) %>% \n",
    "  unnest_tokens(word, text) %>%\n",
    "  mutate(position = row_number()) %>%\n",
    "  filter(!word %in% tm::stopwords(\"en\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input your parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search through all the words of your corpus looking for your word of choice and mark the position of each occurence. Then determine the distance out from each occurence you wish to consider and determine the distance for each occurrence of each word within the chosen distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose your word of interest\n",
    "nearby_words <- all_words %>%\n",
    "  filter(word == \"love\") %>%\n",
    "  #filter(word %in% c(\"father\", \"good\"))\n",
    "  #the script directly above that is commented out is for if you wish to consider more than one word at a time\n",
    "  select(focus_term = word, focus_position = position) %>%\n",
    "  #Choose the distance you wish to analyze\n",
    "  difference_inner_join(all_words, by = c(focus_position = \"position\"), max_dist = 10) %>%\n",
    "  mutate(distance = abs(focus_position - position))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute and display your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 731 x 5\n",
      "    word number maximum_distance minimum_distance average_distance\n",
      "   <chr>  <int>            <dbl>            <dbl>            <dbl>\n",
      " 1  love    105               10                0         2.076190\n",
      " 2  lord     27               10                1         5.148148\n",
      " 3  fear     17               10                1         5.352941\n",
      " 4  time     16                9                2         5.312500\n",
      " 5 great     14                9                1         4.428571\n",
      " 6  know     13               10                1         4.384615\n",
      " 7 think     12               10                1         5.666667\n",
      " 8  dear     10               10                1         5.300000\n",
      " 9   let      9               10                1         6.222222\n",
      "10   man      9               10                1         4.111111\n",
      "# ... with 721 more rows\n"
     ]
    }
   ],
   "source": [
    "words_summarized <- nearby_words %>%\n",
    "  group_by(word) %>%\n",
    "  #group_by(focus_word, word)\n",
    "  #the script directly above that is commented out is for if you are looking at more than one word\n",
    "  summarize(number = n(),\n",
    "            maximum_distance = max(distance),\n",
    "            minimum_distance = min(distance),\n",
    "            average_distance = mean(distance)) %>%\n",
    "  arrange(desc(number))\n",
    "write.csv(words_summarized, file = \"ChooseAnyNameYouWant.csv\")\n",
    "print(words_summarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Much of this code was derived from David Robinson on stackoverflow who helped create the tidytext and fuzzyjoin packages in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
