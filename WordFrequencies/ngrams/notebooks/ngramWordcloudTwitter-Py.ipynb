{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram Twitter Wordcloud\n",
    "This notebook takes you though the steps to create a wordcloud of ngrams (unigrams, bigrams, or trigrams) from tweets in either .csv or .json format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/master/Intro/Python/Py_notebooks/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specificaly for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different packages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary packages for notebook \n",
    "\n",
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download.\n",
    "\n",
    "In your terminal, packages can be installed by simply typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:   \n",
    "\n",
    "- **textblob:** Library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. \n",
    "\n",
    "- **nltk:** Platform for building Python programs to work with human language data.\n",
    "\n",
    "- **re:** Provides regular expression matching operations similar to those found in Perl.\n",
    "\n",
    "- **string:** contains a number of useful constants and classes, as well as some deprecated legacy functions that are also available as methods on strings.\n",
    "\n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "- **collections:** Implements specialized container datatypes providing alternatives to Python's general purpose built-in containers: dict, list, set, and tuple.\n",
    "\n",
    "- **wordcloud:** A simple wordcloud generator in Python.\n",
    "\n",
    "- **PIL:** Stands for Python Imaging Library and adds image processing capabilities.\n",
    "\n",
    "- **numpy:** The fundamental package for scientific computing with Python.\n",
    "\n",
    "- **operator:** Exports a set of functions corresponding to the intrinsic operators of Python.\n",
    "\n",
    "- **glob:** Finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order.\n",
    "\n",
    "- **csv:** Implements classes to read and write tabular data in CSV format.\n",
    "\n",
    "- **json:** Allows for handling of data in JSON format.\n",
    "\n",
    "- **zipfile:** Allows for handling of zipfiles.\n",
    "\n",
    "- **matplotlib:** A Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import wordcloud\n",
    "from wordcloud import STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import operator\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set needed variables\n",
    "This is where you will make some decisions about your data and set the necessary variables. We are doing this so you will not need to make changes in the code further down.\n",
    "\n",
    "First, we need to decide if we want our code to read all the files in a directory or just a single file. If we want all the files in a directory then we set `source` equal to `\"*\"`. This means 'all' and will be added to the file type later in the code. If you want a single file change `\"*\"` to the file name without the \".csv\" or \".json\" at the end. So if you have a file named \"myFile.csv\" you would set `source` equal to `\"myFile\"` without the \".csv\".\n",
    "\n",
    "Next we need to specify what file type our data is saved as. Generally when dealing with Twitter data it will be in either \".json\" or \".csv\" format. For this reason our notebook will only read in \".csv\" or \".json\" files. The variable `fileType` is where you choose either \".csv\" or \".json\" for your `fileType`. So it should look like `fileType = \".csv\"` or `fileType = \".json\"`.\n",
    "\n",
    "Your data can be contained in one file or many. Therefore, you need to state if you want to read in a single document or an entire directory. If you want to read in a single document, then set `singleDoc` equal to **True**. If you want to read in an entire directory of documents then set `singleDoc` equal to **False**.\n",
    "\n",
    "The `nltkStop` is where you determine if you want to use the built in stopword list provided by the NLTK package. They provide stopword lists in multiple languages. If you wish to use this then set `nltkStop` equal to **True**. If you do not, then set `nltkStop` equal to **False**.\n",
    "\n",
    "The `customStop` variable is for if you have a dataset that contains additional stopwords that you would like to read in and have added to the existing `stopWords` list. You do **NOT** need to use the NLTK stopwords list in order to add your own custom list of stopwords. **NOTE: Your custom stopwords file needs to have one word per line as it reads in a line at a time and the full contents of the line is read in and added to the existing stopwords list.** If you have a list of your own then set `customStop` equal to **True**. If you do not have your own custom stopwords list then set `customStop` equal to **False**.\n",
    "\n",
    "The `ng` variable (short for ngram) is where you determine if you want a unigram (basically a word frequency count), a bigram (most common 2 word pairs), or a trigram (most common three word phrases). This will be used in the code further down.\n",
    "\n",
    "The `stopLang` variable is to choose the language of the nltk stopword list you wish to use. It is currently set to `\"english\"`. If you need a different language, simply change `\"english\"` to the anglicized name of the language you wish to use (e.g. \"spanish\" instead of \"espanol\" or \"german\" instead of \"deutsch\"). If you need to see the list of available languages in nltk simply remove the `#` from in front of `#print(\" \".join(stopwords.fileids()))` and run the cell. A list of available languages will display below the cell. \n",
    "\n",
    "The `stopWords = []` is an empty list that will contain the final list of stop words to be removed form your dataset. What ends up in the list depends on whether you set `nltkStop` and/or `customStop` equal to **True** or **False** and if you add any additional words to the list.\n",
    "\n",
    "The `cleanText` variable is another empty list. This is where your document(s) will reside later.\n",
    "\n",
    "The `ngramList` variable is another empty list and is where your resulting ngrams will reside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = \"*\"\n",
    "fileType = \".json\"\n",
    "singleDoc = True\n",
    "nltkStop = True\n",
    "customStop = False\n",
    "ng = 2\n",
    "stopLang = \"english\"\n",
    "stopWords = []\n",
    "cleanText = []\n",
    "ngramList = []\n",
    "\n",
    "#print(\" \".join(stopwords.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File paths\n",
    "Here we are saving different file paths as variables that we need in our code. We again do this so you will not need to make as many changes to the code later. \n",
    "\n",
    "First we use the `os` package above to find our `['HOME']` directory using the `environ` function. This will work for any operating system, so if you decide to try this out on your personal computer instead of ReD, the `homePath` variable will still be the path to your 'home' directory, so no changes are needed.\n",
    "\n",
    "Next, we combine the `homePath` file path with the folder names that lead to where our data is stored. Note that we do not use any file names yet, just the path to the folder. This is because we may want to read in all the files in the directory, or just one, or we may need to access a file in this directory, but need to navigate to another folder to access other data. There are options below for doing both. We save the path as a variable named `dataHome`.\n",
    "\n",
    "Now we add the `homePath` file path to other folder names that lead to a folder where we will want to save any output generated by this code. We again will change the file names for the output in the appropriate cells down below. We save this file path as the variable `dataResults`.\n",
    "\n",
    "Finally, we use an 'if...else' statement to determine what file path we assign to the variable `dataRoot`. If we chose \".csv\" for our `fileType` above then `dataRoot` points to where our \".csv\" data is stored. If we chose \".json\" for `fileType`, then it points to where our \".json\" data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homePath = os.environ['HOME']\n",
    "dataHome = os.path.join(homePath, \"Text-Analysis-master\", \"data\")\n",
    "dataResults = os.path.join(homePath, \"Text-Analysis-master\", \"Output\")\n",
    "if fileType == \".csv\":\n",
    "    dataRoot = os.path.join(dataHome, \"twitter\", \"CSV\", \"parkland\")\n",
    "else:\n",
    "    dataRoot = os.path.join(dataHome, \"twitter\", \"JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "If you set `nltkStop` equal to **True** above then this will add the nltk stopwords list to the empty list named `stopWords`. You should already have chosen your language above, so there is no need to do that here.\n",
    "\n",
    "If you wish to add additional words to the `stopWords` list, add the word in quotes to the list in `stopWords.extend(['the', 'words', 'you', 'want', 'to', 'add'])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "if nltkStop is True:\n",
    "    stopWords.extend(stopwords.words(stopLang))\n",
    "    \n",
    "    stopWords.extend(['amp','rt', 'xo_karmin_ox', 'neveragain', 'ð', 'â', 'ï', 'emma4change', 'governmentshutdown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add own stopword list\n",
    "\n",
    "Here is where your own stopwords list is added if you selected **True** in \"customStop\" above. Here you will need to change the folder names and file name to match your folders and file. Remember to put each folder name in quotes and in the correct path order, always putting the file name including the file extension ('.txt') last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if customStop is True:\n",
    "    stopWordsFilepath = os.path.join(dataHome, \"earlyModernStopword.txt\")\n",
    "\n",
    "    with open(stopWordsFilepath, \"r\",encoding = 'utf-8') as stopfile:\n",
    "        stopWordsCustom = [x.strip() for x in stopfile.readlines()]\n",
    "\n",
    "    stopWords.extend(stopWordsCustom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We need to create a few functions in order to calculate and create a wordcloud. Any time you see `def` that means we are *DE*claring a *F*unction. The `def` is usually followed by the name of the function being created and then in parentheses are the parameters the function requires. After the parentheses is a colon, which closes the declaration, then a bunch of code below which is indented. The indented code is the program statement or statements to be executed. Once you have created your function all you need to do in order to run it is call the function by name and make sure you have included all the required parameters in the parentheses. This allows you to do what the function does without having to write out all the code in the function every time you wish to perform that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning\n",
    "This function does some text cleaning for us and requires the parameter: text (as in what text are you cleaning).\n",
    "\n",
    "Now we come to the statements to be executed. First we lowercase the text or else 'Love' and 'love' will be counted as two different words, so we make them all 'love'. Next we remove URLs by removing any text that starts with 'http' and ending with a space. Then we split the text into individual words. Next we remove any empty space, digits, stopwords, and punctuation. Finally, we return a list of cleaned words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textClean(text):\n",
    "    \n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    tweets = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    tokens = re.split(r'\\W+', tweets )\n",
    "    \n",
    "    # remove empty string\n",
    "    tokens = [t for t in tokens if t]\n",
    "    \n",
    "    # remove digits\n",
    "    tokens = [t for t in tokens if not t.isdigit()]\n",
    "    \n",
    "    # built-in stop words list\n",
    "    tokens = [t for t in tokens if t not in stopWords]\n",
    "        \n",
    "    # remove punctuation\n",
    "    puncts = list(string.punctuation)\n",
    "    puncts.append('--')\n",
    "\n",
    "    tokens = [t for t in tokens if t not in puncts]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Unzip files\n",
    "\n",
    "Here we are unzipping files. Since twitter data can be rather large it is often necessary to compress it into a '.zip' file in order to upload it to places such as GitHub. For this reason, we have setup some code to go in and automatically extract all the items in a compressed '.zip' file so you do't have to and so you don't get errors later. If the data is not in a '.zip' file there is no need to worry, it will not give an error if there are no files ending in '.zip' in your directory.\n",
    "\n",
    "You should not need to make any changes as we use the same variables containing our file paths as above, so if you need to make adjustments to the file paths, you need to make them there, specifcally to the `dataRoot` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "direct = dataRoot\n",
    "allZipFiles = glob.glob(os.path.join(dataRoot, \"*.zip\"))\n",
    "for item in allZipFiles:\n",
    "    fileName = os.path.splitext(direct)[0]\n",
    "    zipRef = zipfile.ZipFile(item, \"r\")\n",
    "    zipRef.extractall(fileName)\n",
    "    zipRef.close()\n",
    "    os.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in .csv files\n",
    "\n",
    "If you chose `\".csv\"` as your `fileType` up above, then the code below reads in \".csv\" files and saves the contents to a dataframe using the Pandas package. It will read in either an entire directory or a single \".csv\" file depending on what you chose for `source` above. \n",
    "\n",
    "Once we have read in the \".csv\" file using the Pandas `read_csv` function, we need to concatenate the \".csv\" files if there are multiple. Because of this it is important that your \".csv\" files have and identical column count and each column has identical header names or you will get errors. If you have a single \".csv\" file then you should be fine for this step. We assign this process to the variable `cc_df` so we can use it later.\n",
    "\n",
    "Now we convert our `cc_df` to a pandas dataframe. This allows for easier manipulation of the data in the next step.\n",
    "\n",
    "The last line is where you will name the column you wish to convert to a list. In between the square brackets you can either put the column header name with quotes around the name, or you can put the column number counting from left to right and starting with 0 instead of 1. Here we are converting the column labeled `['text']` to a list. If you wished to convert the first column to a list the code would look like this: `tweets = cc_df[0].values.tolist()`. Notice the number is 0 and not 1. This is because Python begins counting at 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".csv\":\n",
    "    all_files = glob.glob(os.path.join(dataRoot,source + fileType))     \n",
    "    df_all = (pd.read_csv(f) for f in all_files)\n",
    "    cc_df = pd.concat(df_all, ignore_index=True)\n",
    "    cc_df = pd.DataFrame(cc_df, dtype = 'str')\n",
    "    tweets = cc_df['text'].values.tolist()\n",
    "    content = '\\n'.join(tweets)\n",
    "    cleanTokens = textClean(content)\n",
    "\n",
    "    print('Finished tokenizing text {}\\n'.format(all_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in JSON files\n",
    "\n",
    "If you chose `\".json\"` as your `fileType` up above, then the code below reads in \".json\" files and saves the contents to a dataframe using the Pandas package. It will read in either an entire directory or a single \".json\" file depending on what you chose for `source` above. \n",
    "\n",
    "Once we have read in the \".json\" file we need to append the \".json\" files if there are multiple. Because of this it is important that your \".json\" files have identical keys or you will get errors. If you have a single \".json\" file then you should be fine for this step. \n",
    "\n",
    "Next we append the contents to our empty `tweets=[]` list we created above. Then we convert our tweets list to a dataframe so we can more easily manipulate the data. Each key from the \".json\" content is converted to a header in the dataframe and the values associated with that key now become rows in the corresponding column.\n",
    "\n",
    "In the last line we now convert one column in our dataframe to a list. In between the square brackets you can either put the column header name with quotes around the name, or you can put the column number counting from left to right and starting with 0 instead of 1. Here we are converting the column labeled `['text']` to a list. If you wished to convert the first column to a list the code would look like this: `tweets = df[0].tolist()`. Notice the number is 0 and not 1. This is because Python begins counting at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing text /N/u/klosteda/Carbonate/Text-Analysis-master/data/twitter/JSON/part-m-00000.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if fileType == \".json\":\n",
    "    for filename in glob.glob(os.path.join(dataRoot, source + fileType)):\n",
    "        with open(filename, 'r', encoding = \"utf-8\") as jsonData:\n",
    "            tweets = []\n",
    "            for line in jsonData:\n",
    "                tweets.append(json.loads(line))\n",
    "    df = pd.DataFrame(tweets)\n",
    "    data = df['text'].tolist()\n",
    "    content = '\\n'.join(data)\n",
    "    cleanTokens = textClean(content)\n",
    "\n",
    "    print('Finished tokenizing text {}\\n'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes needed here. This just converts our text to a str object so we can find ngrams later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanText = ' '.join(cleanTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Ngrams\n",
    "\n",
    "We use the textblob package to get ngrams. We use the `ng` variable we created earlier to determine if we are interested in unigrams, bigrams, or trigrams. There should be no reason to make changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(cleanText)\n",
    "\n",
    "if ng == 1: \n",
    "    nGrams = blob.ngrams(n=1)\n",
    "if ng == 2:\n",
    "    nGrams = blob.ngrams(n=2)\n",
    "if ng == 3:\n",
    "    nGrams = blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are converting our ngrams to a list which we can then put into a dataframe to be turned into a wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for wlist in nGrams:\n",
    "   ngramList.append(' '.join(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our dataframe. You won't need to make changes to this code. Just note that we had to go through the dataframe and replace the space between our ngrams with an underscore. This is because the Python wordcloud package has trouble handling the space, so we connect our ngrams with an underscore so that the wordcloud package will see it as one word, but humans can see it as two or three, depending on if you chose bigrams or trigrams above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ngramList)\n",
    "df = df.replace(' ', '_', regex=True)\n",
    "dfCounts = df[0].value_counts()\n",
    "countsDF = pd.DataFrame(dfCounts)\n",
    "countsDF.reset_index(inplace = True)\n",
    "df_C = countsDF.rename(columns={'index':'ngrams',0:'freq'})\n",
    "df_C.set_index(df_C['ngrams'], inplace = True)\n",
    "df_C['ngrams'] = df_C['ngrams'].astype(str)\n",
    "dfNG = df_C.sort_values('freq', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see what our dataframe looks like. If you want to see more, just change the number in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngrams</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngrams</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cspan_blame</th>\n",
       "      <td>cspan_blame</td>\n",
       "      <td>2962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longest_history</th>\n",
       "      <td>longest_history</td>\n",
       "      <td>2805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame_longest</th>\n",
       "      <td>blame_longest</td>\n",
       "      <td>2756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>history_cspan</th>\n",
       "      <td>history_cspan</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government_shutdown</th>\n",
       "      <td>government_shutdown</td>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deplorabletrump_blame</th>\n",
       "      <td>deplorabletrump_blame</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white_house</th>\n",
       "      <td>white_house</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national_park</th>\n",
       "      <td>national_park</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>federal_employees</th>\n",
       "      <td>federal_employees</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>federal_workers</th>\n",
       "      <td>federal_workers</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ngrams  freq\n",
       "ngrams                                            \n",
       "cspan_blame                      cspan_blame  2962\n",
       "longest_history              longest_history  2805\n",
       "blame_longest                  blame_longest  2756\n",
       "history_cspan                  history_cspan  1287\n",
       "government_shutdown      government_shutdown   771\n",
       "deplorabletrump_blame  deplorabletrump_blame   561\n",
       "white_house                      white_house   503\n",
       "national_park                  national_park   452\n",
       "federal_employees          federal_employees   451\n",
       "federal_workers              federal_workers   434"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNG.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot our wordcloud\n",
    "\n",
    "We begin by determining if we want to use a mask to shape the wordcloud. The wordcloud package uses an image file which is refered to as a mask to determine the shape. The resulting wordcloud will be in the shape of the chosen image. If we want to use a mask we need to set `useMask` equal to **True**. \n",
    "\n",
    "We have images to use for making wordcloud shapes and the path in the first line points to the folder the images are stored in. Simply adjust the path to point to where this folder is located on Carbonate for you. Then in the next line choose the name of the image file you wish to use. It is best to choose an image with a lot of contrast between the image and the background, such as a stencil where the image is all black and the background is white.\n",
    "\n",
    "Next we choose the maximum number of ngrams we want in our wordcloud by assigning the number to the variable `maxWrdCnt`.\n",
    "\n",
    "Then we choose a background color and assign it the variable `bgColor`. \n",
    "\n",
    "Next we choose the color of the words in the wordcloud and assign it to the variable `color`. The current color is from the RColorBrewer palette of colors. You can find other color options [here](https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/colorPaletteCheatsheet.pdf). Just put the name of the color selection you want in quotes.\n",
    "\n",
    "Now we choose the figure size. The first number is the width and the second number is the height. Feel free to make changes as needed. We assign the figure size to the variable `FigureSz`.\n",
    "\n",
    "Then we name the output '.png' file our wordcloud will be saved as and assign it to the variable `wcOutputFile`. \n",
    "\n",
    "Next we determine the format of the output file. We already named our file above with a '.png' file type. So we need to make sure this matches that file type. We assign the format to the variable `imgFmt`.\n",
    "\n",
    "Then we choose the resolution of our output image by assigning the dpi resolution we want to the variable `dpi`.\n",
    "\n",
    "Then we create an additional list of stopwords that will remove problematic ngrams. Just remember to type the ngram with an underscore between the two words. Then we remove the row containing the ngram from the database. This keeps you from having to go up and remove single words (and potentially other interesting ngrams) as you can now just remove the specific ngram.\n",
    "\n",
    "Next we give parameters for our wordcloud. and save them as `wc`. Here we have an 'if else' statement that runs one line if we set `useMask` equal to **True** or it will run the other (`else:`) if it is equal to **False**. Any changes to these lines of code were already decided in the variable at the beginning of the cell.\n",
    "\n",
    "Then are the statements for how the wordcloud is displayed. We already chose the figure size above, so we can leave `plt.figure(figsize = figureSz)` alone.  Then we want it to look like what we described in our `wc` variable which is what the `plt.imshow(wc, interpolation = 'bilinear')` statement does. Then we need to state that we are not using an x or y axis by using the `plt.axis(\"off\")` staement. Next we want the layout to be tight instead of spread out so we use the `plt.tight_layout()` statement. Then we state arguments for how we want the wordcloud saved to file (`plt.savefig(os.path.join(dataResults, wcOutputFile), format = imgFmt, dpi = dpi, bbox_inches = 'tight')`), and finally that we want to see the final result displayed in the notebook which is what`plt.show()` does.\n",
    "\n",
    "Run the code and generate your ngram wordcloud!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "useMask = True\n",
    "maskPath = os.path.join(homePath, 'Text-Analysis-master','data','wordcloudMasks')\n",
    "mask = np.array(Image.open(os.path.join(maskPath, \"USA.png\")))\n",
    "maxWrdCnt = 500\n",
    "bgColor = \"black\"\n",
    "color = \"Dark2\"\n",
    "figureSz = (80,40)\n",
    "wcOutputFile = \"twitterNgramWordCloud.png\"\n",
    "imgFmt = \"png\"\n",
    "dpi = 300\n",
    "\n",
    "# Ngram Stopwords\n",
    "stopwords = [\"ngrams\",\"good_lord\",\"come_come\"]\n",
    "text = dfNG[~dfNG['ngrams'].isin(stopwords)]\n",
    "\n",
    "# Wordcloud aesthetics\n",
    "#if useMask is True:    \n",
    "    #wc = wordcloud.WordCloud(background_color = bgColor, max_words = maxWrdCnt, colormap = color, mask = mask).generate_from_frequencies(text['freq'])\n",
    "#else:\n",
    "    #wc = wordcloud.WordCloud(background_color = bgColor, max_words = maxWrdCnt, colormap = color, mask = None).generate_from_frequencies(text['freq'])\n",
    "\n",
    "# show\n",
    "#plt.figure(figsize = figureSz)\n",
    "#plt.imshow(wc, interpolation = 'bilinear')\n",
    "#plt.axis(\"off\")\n",
    "#plt.tight_layout()\n",
    "    \n",
    "# save graph as an image to file\n",
    "#plt.savefig(os.path.join(dataResults, wcOutputFile), format = imgFmt, dpi = dpi, bbox_inches = 'tight')\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## VOILA!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
