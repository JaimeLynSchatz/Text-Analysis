{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram Text Wordcloud\n",
    "\n",
    "The following notebook takes you through the code to create a ngram (unigram, bigram, trigram, et cetera) wordcloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/master/Intro/Python/Py_notebooks/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specificaly for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different packages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary packages for notebook \n",
    "\n",
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download.\n",
    "\n",
    "In your terminal, packages can be installed by simply typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:  \n",
    "\n",
    "- **textblob:** Library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. \n",
    "\n",
    "- **nltk:** Platform for building Python programs to work with human language data.\n",
    "\n",
    "- **re:** Provides regular expression matching operations similar to those found in Perl.\n",
    "\n",
    "- **string:** contains a number of useful constants and classes, as well as some deprecated legacy functions that are also available as methods on strings.\n",
    "\n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "- **collections:** Implements specialized container datatypes providing alternatives to Python's general purpose built-in containers: dict, list, set, and tuple.\n",
    "\n",
    "- **wordcloud:** A simple wordcloud generator in Python.\n",
    "\n",
    "- **PIL:** Stands for Python Imaging Library and adds image processing capabilities.\n",
    "\n",
    "- **numpy:** The fundamental package for scientific computing with Python.\n",
    "\n",
    "- **operator:** Exports a set of functions corresponding to the intrinsic operators of Python.\n",
    "\n",
    "- **glob:** Finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order.\n",
    "\n",
    "- **matplotlib:** A Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import wordcloud\n",
    "from wordcloud import STOPWORDS\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import operator\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File paths\n",
    "Here we are saving as variables different file paths that we need in our code. We do this so that they are easier to call later and so that you can make most of your changes now and not need to make as many changes later. \n",
    "\n",
    "First we use the `os` package above to find our `['HOME']` directory using the `environ` function. This will work for any operating system, so if you decide to try this out on your personal computer instead of ReD, the `homePath` variable will still be the path to your 'home' directory, so no changes are needed.\n",
    "\n",
    "Next, we combine the `homePath` variable with the folder names that lead to where our data is stored. Note that we do not use any file names yet, just the path to the folder. This is because we have the option later to read in a single file or an entire directory. You will want to change the folder names to match your folder names in your file path.\n",
    "\n",
    "Now we add the `homePath` variable to other folder names that lead to a folder where we will want to save any output generated by this code. You again will want to change the file names in the appropriate cells down below to match your own file names. We save this file path as the variable `dataResults`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homePath = os.environ[\"HOME\"]\n",
    "dataHome = os.path.join(homePath, \"Text-Analysis-master\", \"data\", \"shakespeareDated\")\n",
    "dataResults = os.path.join(homePath, \"Text-Analysis-master\", \"Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set needed variables\n",
    "\n",
    "Now we are setting needed variables that will help determine what the code will do farther down. We again do this so that they are easier to call later and so that you can make most of your changes now and not need to make as many changes later.\n",
    "\n",
    "The first variable `data` is set as being equal to `\"*.txt\"`. This is where you determine if you want to read in an entire directory of '.txt' files or a single '.txt' file. If you want the entire directory, leave `data` equal to `\"*.txt\"`. If you want a single document then replace the \"\\*\" with the name of the file, so `data = \"*.txt\"` becomes `data = \"myFileName.txt\"`. \n",
    "\n",
    "If you want to use the stopword list that comes with the nltk package then set `nltkStop` equal to **True**. If you do not wish to use the nltk stopword list then set `nltkStop` equal to **False**.\n",
    "\n",
    "If you have created your own custom stopword list and wish to use that, then set `customStop` equal to **True**. If you do not have your own custom stopword list then set `customStop` equal to **False**.\n",
    "\n",
    "**NOTE: You can use both the nltk and custom stopword lists or you can use neither or just one or the other. You do NOT need to set them both to True or both to False. Use whatever works best for you.**\n",
    "\n",
    "The `ng` variable (short for ngram) is where you determine if you want a unigram (basically a word frequency count), a bigram (most common 2 word pairs), or a trigram (most common three word phrases). This will be used in the code further down.\n",
    "\n",
    "Now we choose the language we will be using for the nltk stopwords list. If you need a different language, simply change `'english'` in the `stopLang` variable to the anglicized name of the language you wish to use (e.g. 'spanish' instead of 'espanol' or 'german' instead of 'deutsch'). If you need to see the list of available languages in nltk simply remove the `#` from in front of `#print(\" \".join(stopwords.fileids()))` on the last line and run the cell. A list of available languages will display below the cell.\n",
    "\n",
    "The `stopWords` variable is simply an empty list (that is what the square brackets [] indicate). This is where the words from the nltk stopword list or your custom stopword list or both combined or neither (depending on what you decide) will reside later on. You do not need to do anything to this line of code.\n",
    "\n",
    "The `cleanText` variable is another empty list. This is where your document(s) will reside later.\n",
    "\n",
    "The `ngramList` variable is another empty list and is where your resulting ngrams will reside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"*.txt\"\n",
    "nltkStop = True\n",
    "customStop = True\n",
    "ng = 2\n",
    "stopLang = 'english'\n",
    "stopWords = []\n",
    "cleanText = []\n",
    "ngramList = []\n",
    "\n",
    "#print(\" \".join(stopwords.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "If you set `nltkStop` equal to **True** above then this will add the nltk stopwords list to the empty list named `stopWords`.\n",
    "\n",
    "If you wish to add additional words to the stopWords list then add the word in quotes to the list in `stopWords.extend(['the', 'words', 'you', 'want', 'to', 'add'])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "if nltkStop is True:\n",
    "    stopWords.extend(stopwords.words(stopLang))\n",
    "\n",
    "    stopWords.extend(['would', 'said', 'says', 'also'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add own stopword list\n",
    "\n",
    "Here is where your own stopwords list is added if you selected **True** in `customStop` above. Here you will need to change the folder names and file name to match your folders and file. Remember to put each folder name in quotes and in the correct order always putting the file name including the file extension (.txt) last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if customStop is True:\n",
    "    stopWordsFilepath = os.path.join(homePath, \"Text-Analysis-DavidBranchV2\", \"data\", \"earlyModernStopword.txt\")\n",
    "\n",
    "    with open(stopWordsFilepath, \"r\",encoding = 'utf-8') as stopfile:\n",
    "        stopWordsCustom = [x.strip() for x in stopfile.readlines()]\n",
    "\n",
    "    stopWords.extend(stopWordsCustom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We need to create a function in order to calculate and create a wordcloud. Any time you see `def` that means we are DEclaring a Function. The `def` is usually followed by the name of the function being created and then in parentheses are the parameters required by the function. After the parentheses is a colon, which closes the declaration, then a bunch of code below which is indented. The indented code is the program statement or statements to be executed. Once you have created your function all you need to do in order to run it is call the function by name and make sure you have included all the required parameters in the parentheses. This allows you to call the function without having to write out all the code in the function every time you wish to perform that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning\n",
    "\n",
    "This function does some text cleaning for us and requires the parameter: text (as in what text are you cleaning).\n",
    "\n",
    "Now we come to the statements to be executed. First we lowercase the text or else 'Love' and 'love' will be counted as two different words, so we make them all 'love'. Then we split the text into individual words and remove empty spaces. Then we remove any digits, stopwords, and punctuation and return a list of cleaned words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textClean(text):\n",
    "    \n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    tokens = re.split(r'\\W+', text )\n",
    "    \n",
    "    # remove empty string\n",
    "    tokens = [t for t in tokens if t]\n",
    "    \n",
    "    # remove digits\n",
    "    tokens = [t for t in tokens if not t.isdigit()]\n",
    "    \n",
    "    # built-in stop words list\n",
    "    tokens = [t for t in tokens if t not in stopWords]\n",
    "        \n",
    "    # remove punctuation\n",
    "    puncts = list(string.punctuation)\n",
    "    puncts.append('--')\n",
    "\n",
    "    tokens = [t for t in tokens if t not in puncts]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Reading in the Text\n",
    "\n",
    "Now we read in the text. You chose earlier whether to read in a single text or the entire directory, so there should be no need to make changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in glob.glob(os.path.join(dataHome, data)):\n",
    "    with open(path, \"r\") as file:\n",
    "         # skip hidden file\n",
    "        if path.startswith('.'):\n",
    "            continue\n",
    "        text = file.read()\n",
    "        cleanText.extend(textClean(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes needed here. This just converts our text to a str object so we can find ngrams later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanTokens = ' '.join(cleanText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Ngrams\n",
    "\n",
    "We use the textblob package to get ngrams. We use the `ng` variable we created earlier to determine if we are interested in unigrams, bigrams, or trigrams. There should be no reason to make changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(cleanTokens)\n",
    "\n",
    "if ng == 1: \n",
    "    nGrams = blob.ngrams(n=1)\n",
    "if ng == 2:\n",
    "    nGrams = blob.ngrams(n=2)\n",
    "if ng == 3:\n",
    "    nGrams = blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are converting our ngrams to a list which we can then put into a dataframe to be turned into a wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for wlist in nGrams:\n",
    "   ngramList.append(' '.join(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our dataframe. You won't need to make changes to this code. Just note that we had to go through the dataframe and replace the space between our ngrams with an underscore. This is because the Python wordcloud package has trouble handling the space, so we connect our ngrams with an underscore so that the wordcloud package will see it as one word, but humans can see it as two or three, depending on if you chose bigrams or trigrams above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ngramList)\n",
    "df = df.replace(' ', '_', regex=True)\n",
    "dfCounts = df[0].value_counts()\n",
    "countsDF = pd.DataFrame(dfCounts)\n",
    "countsDF.reset_index(inplace = True)\n",
    "df_C = countsDF.rename(columns={'index':'ngrams',0:'freq'})\n",
    "df_C.set_index(df_C['ngrams'], inplace = True)\n",
    "df_C['ngrams'] = df_C['ngrams'].astype(str)\n",
    "dfNG = df_C.sort_values('freq', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see what our dataframe looks like. If you want to see more, just change the number in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngrams</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngrams</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>good_lord</th>\n",
       "      <td>good_lord</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come_come</th>\n",
       "      <td>come_come</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come_hither</th>\n",
       "      <td>come_hither</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good_morrow</th>\n",
       "      <td>good_morrow</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good_night</th>\n",
       "      <td>good_night</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come_let</th>\n",
       "      <td>come_let</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lord_lord</th>\n",
       "      <td>lord_lord</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love_love</th>\n",
       "      <td>love_love</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old_man</th>\n",
       "      <td>old_man</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hear_speak</th>\n",
       "      <td>hear_speak</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ngrams  freq\n",
       "ngrams                        \n",
       "good_lord      good_lord   243\n",
       "come_come      come_come   157\n",
       "come_hither  come_hither   112\n",
       "good_morrow  good_morrow   109\n",
       "good_night    good_night    83\n",
       "come_let        come_let    70\n",
       "lord_lord      lord_lord    64\n",
       "love_love      love_love    64\n",
       "old_man          old_man    63\n",
       "hear_speak    hear_speak    59"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNG.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot our wordcloud\n",
    "\n",
    "We begin by determining if we want to use a mask to shape the wordcloud. The wordcloud package uses an image file which is refered to as a mask to determine the shape. The resulting wordcloud will be in the shape of the chosen image. If we want to use a mask we need to set `useMask` equal to **True**. \n",
    "\n",
    "We have images to use for making wordcloud shapes and the path in the first line points to the folder the images are stored in. Simply adjust the path to point to where this folder is located on Carbonate for you. Then in the next line choose the name of the image file you wish to use. It is best to choose an image with a lot of contrast between the image and the background, such as a stencil where the image is all black and the background is white.\n",
    "\n",
    "Next we choose the maximum number of ngrams we want in our wordcloud by assigning the number to the variable `maxWrdCnt`.\n",
    "\n",
    "Then we choose a background color and assign it the variable `bgColor`. \n",
    "\n",
    "Next we choose the color of the words in the wordcloud and assign it to the variable `color`. The current color is from the RColorBrewer palette of colors. You can find other color options [here](https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/colorPaletteCheatsheet.pdf). Just put the name of the color selection you want in quotes.\n",
    "\n",
    "Now we choose the figure size. The first number is the width and the second number is the height. Feel free to make changes as needed. We assign the figure size to the variable `FigureSz`.\n",
    "\n",
    "Then we name the output '.png' file our wordcloud will be saved as and assign it to the variable `wcOutputFile`. \n",
    "\n",
    "Next we determine the format of the output file. We already named our file above with a '.png' file type. So we need to make sure this matches that file type. We assign the format to the variable `imgFmt`.\n",
    "\n",
    "Then we choose the resolution of our output image by assigning the dpi resolution we want to the variable `dpi`.\n",
    "\n",
    "Then we create an additional list of stopwords that will remove problematic ngrams. Just remember to type the ngram with an underscore between the two words. Then we remove the row containing the ngram from the database. This keeps you from having to go up and remove single words (and potentially other interesting ngrams) as you can now just remove the specific ngram.\n",
    "\n",
    "Next we give parameters for our wordcloud. and save them as `wc`. Here we have an 'if else' statement that runs one line if we set `useMask` equal to **True** or it will run the other (`else:`) if it is equal to **False**. Any changes to these lines of code were already decided in the variable at the beginning of the cell.\n",
    "\n",
    "Then are the statements for how the wordcloud is displayed. We already chose the figure size above, so we can leave `plt.figure(figsize = figureSz)` alone.  Then we want it to look like what we described in our `wc` variable which is what the `plt.imshow(wc, interpolation = 'bilinear')` statement does. Then we need to state that we are not using an x or y axis by using the `plt.axis(\"off\")` staement. Next we want the layout to be tight instead of spread out so we use the `plt.tight_layout()` statement. Then we state arguments for how we want the wordcloud saved to file (`plt.savefig(os.path.join(dataResults, wcOutputFile), format = imgFmt, dpi = dpi, bbox_inches = 'tight')`), and finally that we want to see the final result displayed in the notebook which is what`plt.show()` does.\n",
    "\n",
    "Run the code and generate your ngram wordcloud!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "useMask = False\n",
    "maskPath = os.path.join(homePath, 'Text-Analysis-master','data','wordcloudMasks')\n",
    "mask = np.array(Image.open(os.path.join(maskPath, \"Shakespeare.png\")))\n",
    "maxWrdCnt = 500\n",
    "bgColor = \"black\"\n",
    "color = \"Dark2\"\n",
    "figureSz = (80,40)\n",
    "wcOutputFile = \"ngramWordCloud.png\"\n",
    "imgFmt = \"png\"\n",
    "dpi = 300\n",
    "\n",
    "# Ngram Stopwords\n",
    "stopwords = [\"ngrams\",\"good_lord\",\"come_come\"]\n",
    "text = dfNG[~dfNG['ngrams'].isin(stopwords)]\n",
    "\n",
    "# Wordcloud aesthetics\n",
    "#if useMask is True:    \n",
    "    #wc = wordcloud.WordCloud(background_color = bgColor, max_words = maxWrdCnt, colormap = color, mask = mask).generate_from_frequencies(text['freq'])\n",
    "#else:\n",
    "    #wc = wordcloud.WordCloud(background_color = bgColor, max_words = maxWrdCnt, colormap = color, mask = None).generate_from_frequencies(text['freq'])\n",
    "\n",
    "# show\n",
    "#plt.figure(figsize = figureSz)\n",
    "#plt.imshow(wc, interpolation = 'bilinear')\n",
    "#plt.axis(\"off\")\n",
    "#plt.tight_layout()\n",
    "    \n",
    "# save graph as an image to file\n",
    "#plt.savefig(os.path.join(dataResults, wcOutputFile), format = imgFmt, dpi = dpi, bbox_inches = 'tight')\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## VOILA!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
