library(devtools)
library(twitteR)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweets <- searchTwitter("#PorteOuverte", n=5000)
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
tweets.df$text <- sapply(tweets.df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, tolower, mc.cores=1)
myCorpus <- tm_map(myCorpus, removePunctuation, mc.cores=1)
myCorpus <- tm_map(myCorpus, removeNumbers, mc.cores=1)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
for(i in 1:5) {
cat(paste("[[", i, "]] ", sep = ""))
writeLines(myCorpus[[i]])
}
library(devtools)
library(twitteR)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweets <- searchTwitter("#PorteOuverte", n=5000)
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, tolower, mc.cores=1)
myCorpus <- tm_map(myCorpus, removePunctuation, mc.cores=1)
myCorpus <- tm_map(myCorpus, removeNumbers, mc.cores=1)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
library(devtools)
library(twitteR)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweets <- searchTwitter("#PorteOuverte", n=5000)
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
tweets.df$text <- sapply(tweets.df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, tolower, mc.cores=1)
myCorpus <- tm_map(myCorpus, removePunctuation, mc.cores=1)
myCorpus <- tm_map(myCorpus, removeNumbers, mc.cores=1)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
for(i in 1:5) {
cat(paste("[[", i, "]] ", sep = ""))
writeLines(myCorpus[[i]])
}
View(tweets.df)
stemCompletion_mod <- function(x,dict) {
PlainTextDocument(stripWhitespace(paste(stemCompletion(unlist(strsplit(as.character(x)," ")), dictionary = dict, type = "shortest"), sep = "", collapse = " ")))
}
myCorpus <- lapply(myCorpus, stemCompletion_mod, myCorpusCopy)
install.packages("RWeka")
install.packages("rJava")
install.packages("RWekajars")
library(devtools)
library(twitteR)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweets <- searchTwitter("#PorteOuverte", n=5000)
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
tweets.df$text <- sapply(tweets.df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, tolower, mc.cores=1)
myCorpus <- tm_map(myCorpus, removePunctuation, mc.cores=1)
myCorpus <- tm_map(myCorpus, removeNumbers, mc.cores=1)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument)
for(i in 1:5) {
cat(paste("[[", i, "]] ", sep = ""))
writeLines(myCorpus[[i]])
}
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy, mc.cores=1)
library(devtools)
library(twitteR)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweets <- searchTwitter("#PorteOuverte", n=5000)
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
tweets.df$text <- sapply(tweets.df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, tolower, mc.cores=1)
myCorpus <- tm_map(myCorpus, removePunctuation, mc.cores=1)
myCorpus <- tm_map(myCorpus, removeNumbers, mc.cores=1)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
for(i in 1:5) {
cat(paste("[[", i, "]] ", sep = ""))
writeLines(myCorpus[[i]])
}
tdm <- TermDocumentMatrix(myCorpus, control= list(wordLengths = c(1, Inf)))
tdm <- TermDocumentMatrix(myCorpus, control= list(wordLengths = c(1, Inf)),  mc.cores=1)
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)), mc.cores=1)
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
library(devtools)
library(twitteR)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweets <- searchTwitter("#PorteOuverte", since = 2015-11-13, until = 2015-11-16)
tweets <- searchTwitter("#PorteOuverte", since = "2015-11-13", until = "2015-11-16")
num.tweets <- length(tweets)
num.tweets
tweets <- searchTwitter("#PorteOuverte", since = "2015-11-13", until = "2015-11-16", n=1000)
tweets <- searchTwitter("#PorteOuverte", since = "2015-11-13", until = "2015-11-16", n=5000)
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
View(tweets.df)
tweets[1:5]
tweets[1:20]
tweets <- searchTwitter("#PorteOuverte", since = "2015-11-13", until = "2015-11-15", n=5000)
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
View(tweets.df)
tweets.df$text <- sapply(tweets.df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
View(tweets.df)
tweets <- searchTwitter("#PorteOuverte", since = "2015-11-13", until = "2015-11-15", n=10000)
tweets <- searchTwitter("#PorteOuverte", since = "2015-11-13", until = "2015-11-15", n=7500)
tweets <- searchTwitter("#PorteOuverte", since = "2015-11-13", until = "2015-11-15", n=5000)
library(devtools)
library(twitteR)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweets <- searchTwitter("#PorteOuverte", since = "2015-11-13", until = "2015-11-15", n=5000)
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
tweets.df$text <- sapply(tweets.df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
myCorpus <- Corpus(VectorSource(tweets.df$text))
library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, PlainTextDocument)
myCorpus <- tm_map(myCorpus, tolower, mc.cores=1)
myCorpus <- tm_map(myCorpus, removePunctuation, mc.cores=1)
myCorpus <- tm_map(myCorpus, removeNumbers, mc.cores=1)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords("english"), "available", "via")
myStopwords <- setdiff(myStopwords, c("r", "big"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)), mc.cores=1)
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
tdm <- TermDocumentMatrix(myCorpus)
library(devtools)
library(twitteR)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
delta.tweets = searchTwitter('@delta', n=1500)
length(delta.tweets)
class(delta.tweets)
delta.tweets[1]
install.packages("plyr")
delta.text = laply(delta.tweets, function(t) t$getText())
library(plyr)
library(devtools)
library(twitteR)
library(plyr)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
library(devtools)
library(twitteR)
library(plyr)
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
delta.tweets = searchTwitter('@delta', n=1500)
length(delta.tweets)
class(delta.tweets)
delta.tweets[1]
delta.text = laply(delta.tweets, function(t) t$getText())
length(delta.text)
head(delta.text, 5)
class(delta.text)
install.packages("NLP")
.rs.enableRStudioConnectUI(TRUE)
install.packages("shiny")
install.packages("shiny")
library(shiny)
shiny::runApp('Documents/IU/CyberDH/app1')
runApp('Documents/IU/CyberDH/app1')
runApp('Documents/IU/CyberDH/app1')
runApp('Documents/IU/CyberDH/app1')
knit_with_parameters('~/Documents/IU/CyberDH/Text_Analysis/RNotebooks/sentPolitical.Rmd')
shiny::runApp('Documents/IU/CyberDH/Text_Analysis/Shiny/Twitter')
runApp('Documents/IU/CyberDH/Text_Analysis/Shiny/TwitterSent')
runApp('Documents/IU/CyberDH/Text_Analysis/Shiny/TwitterSent')
library(wordcloud)
library(qdap)
library(RColorBrewer)
setwd("~/Desktop/R/Text_Analysis/data/twitter/")
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
prince.tweets = searchTwitter('Prince', n=5000)
library(wordcloud)
library(qdap)
library(RColorBrewer)
setwd("~/Documents/IU/CyberDH/Text_Analysis/RScripts")
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
library(wordcloud)
library(qdap)
library(RColorBrewer)
library(twitteR)
setwd("~/Documents/IU/CyberDH/Text_Analysis/RScripts")
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
prince.tweets = searchTwitter('Prince', n=5000)
saveRDS(prince.tweets, "princeTweets.RData")
prince.text = laply(prince.tweets, function(t) t$getText())
library(plyr)
prince.text = laply(prince.tweets, function(t) t$getText())
Strip URLS
prince.text=gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", prince.text)
#Strip punctuation
prince.text=gsub( "[^[:alnum:] ]", "", prince.text )
#Split into words
words <-strsplit(prince.text, "\\W+", perl=TRUE)
# #Remove common words
words=rm_stopwords(words,c(Top100Words))
#Get rid of empty elements
words=words[lapply(words,length)>0]
#Flatten the list of lists
words=unlist(words,recursive = FALSE)
#Convert to a sorted frequency table
words=sort(table(words),decreasing=T)
freqs=as.vector(words)
words=names(words)
cols <- colorRampPalette(brewer.pal(12,"Paired"))(500)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=100, rot.per=0,
colors=cols)
words=rm_stopwords(words,c(Top100Words,"rt", "amp", "easter", "https"))
#Get rid of empty elements
words=words[lapply(words,length)>0]
#Flatten the list of lists
words=unlist(words,recursive = FALSE)
#Convert to a sorted frequency table
words=sort(table(words),decreasing=T)
freqs=as.vector(words)
words=names(words)
cols <- colorRampPalette(brewer.pal(12,"Paired"))(500)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=100, rot.per=0,
colors=cols)
prince.text = laply(prince.tweets, function(t) t$getText())
#Strip URLS
prince.text=gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", prince.text)
#Strip punctuation
prince.text=gsub( "[^[:alnum:] ]", "", prince.text )
#Split into words
words <-strsplit(prince.text, "\\W+", perl=TRUE)
# #Remove common words
words=rm_stopwords(words,c(Top100Words,"rt", "amp", "easter", "https"))
#Get rid of empty elements
words=words[lapply(words,length)>0]
#Flatten the list of lists
words=unlist(words,recursive = FALSE)
#Convert to a sorted frequency table
words=sort(table(words),decreasing=T)
freqs=as.vector(words)
words=names(words)
cols <- colorRampPalette(brewer.pal(12,"Paired"))(500)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=100, rot.per=0,
colors=cols)
library(wordcloud)
library(qdap)
library(RColorBrewer)
library(twitteR)
library(plyr)
setwd("~/Documents/IU/CyberDH/Text_Analysis")
prince.tweets <- load(file = "data/princeTweets.RData")
prince.tweets <- load(file = "data/princeTweets.RData")
library(wordcloud)
library(qdap)
library(RColorBrewer)
library(twitteR)
library(plyr)
setwd("~/Documents/IU/CyberDH/Text_Analysis")
api_key <- "akbmHVWwpoxSUIWprIrEx0Cqo"
api_secret <- "pzkXmLBhV7jUKJKHXKN5Zz43evzn12tbUTL95muq6tYBZ08MAn"
access_token <- "285932503-yymLCmhZmFAY2N1YcgBHGULyMMWviWauQIxD6LvS"
access_token_secret <- "CRQirUWnRX1dRE75lELlUA7JryGao3F31VEYX5qm3pIg0"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
prince.tweets = searchTwitter('Prince', n=5000)
saveRDS(prince.tweets, "data/princeTweets.RData")
prince.text = laply(prince.tweets, function(t) t$getText())
#Strip URLS
prince.text=gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", prince.text)
#Strip punctuation
prince.text=gsub( "[^[:alnum:] ]", "", prince.text )
#Split into words
words <-strsplit(prince.text, "\\W+", perl=TRUE)
# #Remove common words
words=rm_stopwords(words,c(Top100Words,"rt", "amp", "easter", "https"))
#Get rid of empty elements
words=words[lapply(words,length)>0]
#Flatten the list of lists
words=unlist(words,recursive = FALSE)
#Convert to a sorted frequency table
words=sort(table(words),decreasing=T)
freqs=as.vector(words)
words=names(words)
cols <- colorRampPalette(brewer.pal(12,"Paired"))(500)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=100, rot.per=0,
colors=cols)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=50, rot.per=0,
colors=cols)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=50, rot.per=0,
colors=cols)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=75, rot.per=0,
colors=cols)
prince.text = laply(prince.tweets, function(t) t$getText())
#Strip URLS
prince.text=gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", prince.text)
#Strip punctuation
prince.text=gsub( "[^[:alnum:] ]", "", prince.text )
#Split into words
words <-strsplit(prince.text, "\\W+", perl=TRUE)
# #Remove common words
words=rm_stopwords(words,c(Top100Words,"rt", "amp", "easter", "https", "futuredilf", "la", "s", "el", "en", "m"))
#Get rid of empty elements
words=words[lapply(words,length)>0]
#Flatten the list of lists
words=unlist(words,recursive = FALSE)
#Convert to a sorted frequency table
words=sort(table(words),decreasing=T)
freqs=as.vector(words)
words=names(words)
cols <- colorRampPalette(brewer.pal(12,"Paired"))(500)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=75, rot.per=0,
colors=cols)
prince.text = laply(prince.tweets, function(t) t$getText())
#Strip URLS
prince.text=gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", prince.text)
#Strip punctuation
prince.text=gsub( "[^[:alnum:] ]", "", prince.text )
#Split into words
words <-strsplit(prince.text, "\\W+", perl=TRUE)
# #Remove common words
words=rm_stopwords(words,c(Top100Words,"rt", "amp", "easter", "https", "futuredilf", "la", "s", "el", "en", "m", "de", "2"))
#Get rid of empty elements
words=words[lapply(words,length)>0]
#Flatten the list of lists
words=unlist(words,recursive = FALSE)
#Convert to a sorted frequency table
words=sort(table(words),decreasing=T)
freqs=as.vector(words)
words=names(words)
cols <- colorRampPalette(brewer.pal(12,"Paired"))(500)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=75, rot.per=0,
colors=cols)
prince.text = laply(prince.tweets, function(t) t$getText())
#Strip URLS
prince.text=gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "", prince.text)
#Strip punctuation
prince.text=gsub( "[^[:alnum:] ]", "", prince.text )
#Split into words
words <-strsplit(prince.text, "\\W+", perl=TRUE)
# #Remove common words
words=rm_stopwords(words,c(Top100Words,"rt", "amp", "easter", "https", "futuredilf", "la", "s", "el", "en", "m", "de", "2"))
#Get rid of empty elements
words=words[lapply(words,length)>0]
#Flatten the list of lists
words=unlist(words,recursive = FALSE)
#Convert to a sorted frequency table
words=sort(table(words),decreasing=T)
freqs=as.vector(words)
words=names(words)
cols <- colorRampPalette(brewer.pal(12,"Paired"))(500)
wordcloud(words,freqs,scale=c(3,1),min.freq=3,max.words=75, rot.per=0,
colors=cols)
setwd("~/Documents/IU/CyberDH/Text_Analysis")
score.sentiment = function(tweets, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
#REMOVE EMOJIS!
tweet = iconv(tweet, "ASCII", "UTF-8", sub="")
tweet.lower = tolower(tweet)
word.list = str_split(tweet.lower, '\\s+')
words = unlist(word.list)
#compare our words to the dictionaries of positive and negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
#match returns a position of the matched term or NA, but we just want the TRUE/FALSE, not NA
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
#the score of each tweet is the sum of the positive matches minus the sum of the negative matches
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress = .progress)
scores.df = data.frame(score=scores, text = tweets)
return(scores.df)
}
lex.pos = scan('opinionLexicon/positive-words.txt', what='character', comment.char = ';')
lex.neg = scan('opinionLexicon/negative-words.txt', what='character', comment.char = ';')
# add words using the c() [combine] function
pos.words = c(lex.pos)
neg.words = c(lex.neg)
lex.pos = scan('data/opinionLexicon/positive-words.txt', what='character', comment.char = ';')
lex.neg = scan('data/opinionLexicon/negative-words.txt', what='character', comment.char = ';')
# add words using the c() [combine] function
pos.words = c(lex.pos)
neg.words = c(lex.neg)
score.sentiment = function(tweets, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('[[:punct:]]', '', tweet)
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet)
#REMOVE EMOJIS!
tweet = iconv(tweet, "ASCII", "UTF-8", sub="")
tweet.lower = tolower(tweet)
word.list = str_split(tweet.lower, '\\s+')
words = unlist(word.list)
#compare our words to the dictionaries of positive and negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
#match returns a position of the matched term or NA, but we just want the TRUE/FALSE, not NA
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
#the score of each tweet is the sum of the positive matches minus the sum of the negative matches
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress = .progress)
scores.df = data.frame(score=scores, text = tweets)
return(scores.df)
}
result = score.sentiment(prince.text, pos.words, neg.words)
head(result)
colnames(result)
rownames(result)
result[,'score']
hist(result$score)
head(result)
q = qplot(result$score, main = "Sentiment of Prince on Twitter", xlab= "Valence of Sentiment (Tweet Score)", ylab="Count (Tweets)")
q = q + theme_bw()
q
library(ggplot2)
library(ggplot2)
q = qplot(result$score, main = "Sentiment of Prince on Twitter", xlab= "Valence of Sentiment (Tweet Score)", ylab="Count (Tweets)")
q = q + theme_bw()
q
prince.tweets <- readRDS("data/princeTweets.RData")
