install.packages("shiny")
install.packages("SnowballC")
install.packages("splines")
install.packages("survival")
library("shiny", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library("knitr", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
source('~/Desktop/JockersTextAnalysis.R/Rscripts/chap03-Main.R')
list.files()
devtools::installinstall_github('XsedeScienceGateways/TAG')
install.packages("devtools")
library("devtools", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
devtools::install_github('XsedeScienceGateways/TAG')
updateR()
devtools::install_github('XsedeScienceGateways/TAG')
install_github("cpsievert/LDAvis")
install_github("wrathematics/ngram")
install_github("XSEDEScienceGateways/textgateway")
install.packages("dplyr")
install_github("XSEDEScienceGateways/textgateway")
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
# to data.frame
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
# to text corpus
tweets.corpus <- Corpus(VectorSource(tweets.df$text))
# transformations ####
# lower case
tweets.corpus <- tm_map(tweets.corpus, tolower)
# remove punctuation
tweets.corpus <- tm_map(tweets.corpus, removePunctuation)
# remove numbers
tweets.corpus <- tm_map(tweets.corpus, removeNumbers)
# remove URLs
removeURLs <- function(x) gsub("http[[:alnum:]]*", "", x)
tweets.corpus <- tm_map(tweets.corpus, removeURLs)
# add two stop words, 'available' and 'via'
myStopWords <- c(stopwords('english'), "available", "via")
# remove 'tweetsomy' and 'apps' from stopwords
myStopWords <- setdiff(myStopWords, c("tweetsomy", "apps"))
# remove stopwords from corpus
tweets.corpus <- tm_map(tweets.corpus, removeWords, myStopWords)
save("tweets.corpus", file="tweets.corpus.RData")
# view effect
writeLines(strwrap(tweets.corpus[[4]], width=73))
# stemming ####
library(SnowballC)
# copy
tweets.corpus.copy <- tweets.corpus
# stem
tweets.corpus <- tm_map(tweets.corpus, stemDocument)
inspect(tweets.corpus[4])
# stem completition
tweets.corpus <- tm_map(tweets.corpus, stemCompletion,
dictionary=tweets.corpus.copy)
save(tweets.corpus, file="tweets.corpus.SC.RData")
# to TD matrix ####
library(tm)
# tdm
tweets.tdm <- TermDocumentMatrix(tweets.corpus, control=list(wordLengths=c(1, Inf)))
# print terms
dimnames(tweets.tdm)$Terms
save(tweets.tdm, file="tweets.tdm.RData")
# frequent terms
which(apply(tweets.tdm, 1, sum) > 20)
findFreqTerms(tweets.tdm, lowfreq=20)
findAssocs(tweets.tdm, "mandela", 0.25)
findAssocs(tweets.tdm, "recent", 0.25)
# word cloud ####
library(wordcloud)
tweets.matrix <- as.matrix(tweets.tdm)
wordFreq.sort <- sort(rowSums(tweets.matrix), decreasing=T)
save(tweets.matrix, file="tweets.matrix.RData")
# wcloud
set.seed(1234)
grayLevels <- gray( (wordFreq.sort + 10) / (max(wordFreq.sort) + 10))
word.cloud <- wordcloud(words=names(wordFreq.sort), freq=wordFreq.sort,
min.freq=3, random.order=F, colors=grayLevels)
# tweetsomist, weeks, america, new, year, china, mandela, december
#\-------------hclust-----------\####
# hierarchial clustering of terms ####
setwd("~/Documents/R/mining/twitter")
load("tweets.tdm.RData")
# remove sparse terms ####
tweets.tdm2 <- removeSparseTerms(tweets.tdm, sparse=0.95)
tweets.matrix2 <- as.matrix(tweets.tdm2)
# hclust ####
distMatrix <- dist(scale(tweets.matrix2))
tweets.fit <- hclust(distMatrix, method="ward")
# plot dendrogram ####
plot(tweets.fit, cex=0.9, hang=-1,
main="Word Cluster Dendrogram")
# cut tree
rect.hclust(tweets.fit, k=5)
(tweets.groups <- cutree(tweets.fit, k=5))
# conversion ####
library(tm)
# to data.frame
tweets.df <- do.call("rbind", lapply(tweets, as.data.frame))
# to text corpus
tweets.corpus <- Corpus(VectorSource(tweets.df$text))
# transformations ####
# lower case
tweets.corpus <- tm_map(tweets.corpus, tolower, mc.cores=1)
# remove punctuation
tweets.corpus <- tm_map(tweets.corpus, removePunctuation)
# remove numbers
tweets.corpus <- tm_map(tweets.corpus, removeNumbers)
# remove URLs
removeURLs <- function(x) gsub("http[[:alnum:]]*", "", x)
tweets.corpus <- tm_map(tweets.corpus, removeURLs)
# add two stop words, 'available' and 'via'
myStopWords <- c(stopwords('english'), "available", "via")
# remove 'tweetsomy' and 'apps' from stopwords
myStopWords <- setdiff(myStopWords, c("tweetsomy", "apps"))
# remove stopwords from corpus
tweets.corpus <- tm_map(tweets.corpus, removeWords, myStopWords)
save("tweets.corpus", file="tweets.corpus.RData")
# view effect
writeLines(strwrap(tweets.corpus[[4]], width=73))
# stemming ####
library(SnowballC)
# copy
tweets.corpus.copy <- tweets.corpus
# stem
tweets.corpus <- tm_map(tweets.corpus, stemDocument)
inspect(tweets.corpus[4])
# stem completition
tweets.corpus <- tm_map(tweets.corpus, stemCompletion,
dictionary=tweets.corpus.copy)
save(tweets.corpus, file="tweets.corpus.SC.RData")
# to TD matrix ####
library(tm)
# tdm
tweets.tdm <- TermDocumentMatrix(tweets.corpus, control=list(wordLengths=c(1, Inf)))
# print terms
dimnames(tweets.tdm)$Terms
save(tweets.tdm, file="tweets.tdm.RData")
# frequent terms
which(apply(tweets.tdm, 1, sum) > 20)
findFreqTerms(tweets.tdm, lowfreq=20)
findAssocs(tweets.tdm, "mandela", 0.25)
findAssocs(tweets.tdm, "recent", 0.25)
# word cloud ####
library(wordcloud)
tweets.matrix <- as.matrix(tweets.tdm)
wordFreq.sort <- sort(rowSums(tweets.matrix), decreasing=T)
save(tweets.matrix, file="tweets.matrix.RData")
# wcloud
set.seed(1234)
grayLevels <- gray( (wordFreq.sort + 10) / (max(wordFreq.sort) + 10))
word.cloud <- wordcloud(words=names(wordFreq.sort), freq=wordFreq.sort,
min.freq=3, random.order=F, colors=grayLevels)
# tweetsomist, weeks, america, new, year, china, mandela, december
#\-------------hclust-----------\####
# hierarchial clustering of terms ####
setwd("~/Documents/R/mining/twitter")
load("tweets.tdm.RData")
# remove sparse terms ####
tweets.tdm2 <- removeSparseTerms(tweets.tdm, sparse=0.95)
tweets.matrix2 <- as.matrix(tweets.tdm2)
# hclust ####
distMatrix <- dist(scale(tweets.matrix2))
tweets.fit <- hclust(distMatrix, method="ward")
# plot dendrogram ####
plot(tweets.fit, cex=0.9, hang=-1,
main="Word Cluster Dendrogram")
# cut tree
rect.hclust(tweets.fit, k=5)
(tweets.groups <- cutree(tweets.fit, k=5))
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/Desktop/R/Text_Analysis/shiny/TermFreq2/app.R')
.rs.enableRStudioConnectUI(TRUE)
source('~/Desktop/R/RTwitter/hashtagTwitter.R')
source('~/Desktop/Text-Analysis/RScripts/wordcloudPlainText.R')
setwd("~/Desktop/Text-Analysis/RScripts")
source('~/Desktop/Text-Analysis/RScripts/wordcloudPlainText.R')
source('~/Desktop/Text-Analysis/RScripts/wordcloudPlainText.R')
wordcloud(corpus,random.order=FALSE,scale=c(4,1),rot.per=0,
max.words=75,colors=brewer.pal(8, "Dark2"))
install.packages("qdapDictionaries")
install.packages("qdapRegex")
install.packages("qdapTools")
install.packages("qdap")
source('~/Desktop/R/Text_AnalysisNotGIT/RScripts/wordcloudTwitter.R')
source('~/Desktop/R/Text_AnalysisNotGIT/RScripts/wordcloudTwitter.R')
source('~/Desktop/R/Text_AnalysisNotGIT/RScripts/wordcloudTwitter.R')
